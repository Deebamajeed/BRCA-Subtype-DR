{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "y8IxbuI2oLfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca1623d-791c-4f3b-e3fe-25a07cbab49f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/MyDrive/DeepCDR-master.zip"
      ],
      "metadata": {
        "id": "PWEesj3UoP_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm /content/DeepCDR-master/data/CCLE/ge*"
      ],
      "metadata": {
        "id": "M3FaL1OVoQsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358cb2fe-15ab-4730-e044-7e7d0cafc931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/DeepCDR-master/data/CCLE/ge*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rm /content/DeepCDR-master/prog/layers/graph.py"
      ],
      "metadata": {
        "id": "azZXx5nHojIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/gdrive/MyDrive/multiomics/drug_DeepMO/* /content/DeepCDR-master/data/CCLE/"
      ],
      "metadata": {
        "id": "1zwGNE83oQvg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2efcb4d-d291-47ef-bc1b-d2012f246fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: -r not specified; omitting directory '/content/gdrive/MyDrive/multiomics/drug_DeepMO/hyperparam'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/gdrive/MyDrive/datafiles/graph.py /content/DeepCDR-master/prog/layers"
      ],
      "metadata": {
        "id": "VoWa2QKxoQ1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WsBARNIENNsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/DeepCDR-master/prog/')"
      ],
      "metadata": {
        "id": "sN4YEqmYo-Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hyperas"
      ],
      "metadata": {
        "id": "yJlcYloBpBVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import sys"
      ],
      "metadata": {
        "id": "oxhZD73EpDnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/gdrive/MyDrive/multiomics/drug_DeepMO/hyperparam/funs_hypetune_cdr.py /content/DeepCDR-master/prog"
      ],
      "metadata": {
        "id": "Ivd4FaLKMwni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/gdrive/MyDrive/multiomics/drug_DeepMO/hyperparam/run_hypetune_DeepCDR_classify.py /content/DeepCDR-master/prog"
      ],
      "metadata": {
        "id": "P8_TAcYqNQpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/DeepCDR-master/prog/run_hypetune_DeepCDR_classify.py"
      ],
      "metadata": {
        "id": "RfXuxU5gpHax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bbcc6d-5736-4331-fb0d-58bb5a94eb1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    import argparse\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import random, os, sys\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import csv\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from scipy import stats\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import time\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn import metrics\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.metrics import roc_auc_score\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn import preprocessing\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras.backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Model, Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import load_model\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Input, InputLayer, Multiply, ZeroPadding2D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Conv2D, MaxPooling2D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dense, Activation, Dropout, Flatten, Concatenate\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import BatchNormalization\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Lambda\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import optimizers, utils\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.constraints import max_norm\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import regularizers\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, History\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.optimizers import Adam, SGD\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import model_from_json\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import tensorflow as tf\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.metrics import average_precision_score\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.metrics import mean_squared_error, r2_score\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from scipy.stats import pearsonr\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from funs_hypetune_cdr import *\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import scipy.sparse as sp\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import argparse\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib.pyplot as plt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras.backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Model, Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Input, InputLayer, Multiply, ZeroPadding2D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dense, Activation, Dropout, Flatten, Concatenate\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import BatchNormalization\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Lambda\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dropout, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Model\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.optimizers import Adam\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.regularizers import l2\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from layers.graph import GraphLayer, GraphConv\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import RepeatedKFold\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform, quniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import tempfile\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'layer_1_size': hp.quniform('layer_1_size', 12, 256, 4),\n",
            "        'l1_dropout': hp.uniform('l1_dropout', 0.001, 0.7),\n",
            "        'optimizer': hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
            "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
            "    }\n",
            "\n",
            ">>> Functions\n",
            "  1: def compute_accuracy(y_true, y_pred):\n",
            "  2:     pred = y_pred.ravel() < 0.5\n",
            "  3:     return np.mean(pred == y_true)\n",
            "  4: \n",
            "  5: \n",
            ">>> Data\n",
            "  1: \n",
            "  2: DPATH = '/content/DeepCDR-master/data'  \n",
            "  3: \n",
            "  4: Genomic_mutation_file = '%s/CCLE/genomic_mutation_34673_demap_features_auto.csv'%DPATH\n",
            "  5: \n",
            "  6: Gene_expression_file = '%s/CCLE/genomic_expression_561celllines_697genes_demap_features_auto.csv'%DPATH\n",
            "  7: Methylation_file = '%s/CCLE/genomic_methylation_561celllines_808genes_demap_features_auto.csv'%DPATH\n",
            "  8: mutation_feature, gexpr_feature,methylation_feature, Y = MetadataGenerate(Genomic_mutation_file,Gene_expression_file,Methylation_file,False)\n",
            "  9: \n",
            " 10: X2=np.matrix(mutation_feature)\n",
            " 11: X3=np.matrix(gexpr_feature)\n",
            " 12: X4=np.matrix(methylation_feature)\n",
            " 13: Y=np.matrix(Y)\n",
            " 14: cv = RepeatedKFold(n_splits=7, n_repeats=1, random_state=1)\n",
            " 15: for train_ix,test_ix in cv.split(X2):\n",
            " 16:   print(train_ix)\n",
            " 17:   X2_train,X3_train,X4_train,X2_test,X3_test,X4_test= X2[train_ix],X3[train_ix],X4[train_ix],X2[test_ix],X3[test_ix],X4[test_ix]\n",
            " 18:   Y_train,Y_test = Y[train_ix], Y[test_ix]\n",
            " 19: X_train=[X2_train,X3_train,X4_train]\n",
            " 20: X_test=[X2_test,X3_test,X4_test]\n",
            " 21: print(X_train[2].shape)\n",
            " 22: print(Y_train.shape)\n",
            " 23: \n",
            " 24: \n",
            " 25: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     \"\"\"\n",
            "   4:     Model providing function:\n",
            "   5: \n",
            "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
            "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
            "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
            "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
            "  10:     The last one is optional, though recommended, namely:\n",
            "  11:         - model: specify the model just created so that we can later use it again.\n",
            "  12:     \"\"\"\n",
            "  13:     print(\"AM BACK\")\n",
            "  14:     regr=True\n",
            "  15:     layer_1_size = space['layer_1_size']\n",
            "  16:     l1_dropout = space['l1_dropout']\n",
            "  17:     params = {\n",
            "  18:         'l1_size': layer_1_size,\n",
            "  19:         'l1_dropout': l1_dropout\n",
            "  20:     }\n",
            "  21:     print(X_train[0].shape[-1])\n",
            "  22:     print(Y_test.shape[-1])\n",
            "  23:     print(\"??????????????????????\")\n",
            "  24:     mut_input = Input(shape=(X_train[0].shape[-1],))\n",
            "  25:     gexpr_input = Input(shape=(X_train[1].shape[-1],))\n",
            "  26:     methy_input = Input(shape=(X_train[2].shape[-1],))\n",
            "  27:     \n",
            "  28:     y_dim=Y_train.shape[1]\n",
            "  29:     \n",
            "  30:     x_mut = Dense(16)(mut_input)\n",
            "  31:     x_mut = Activation('tanh')(x_mut)\n",
            "  32:     x_mut = BatchNormalization()(x_mut)\n",
            "  33:     x_mut = Dropout(0.1)(x_mut)\n",
            "  34:     x_mut = Dense((y_dim),activation='relu')(x_mut)\n",
            "  35:         #gexp feature\n",
            "  36:     x_gexpr = Dense(16)(gexpr_input)\n",
            "  37:     x_gexpr = Activation('tanh')(x_gexpr)\n",
            "  38:     x_gexpr = BatchNormalization()(x_gexpr)\n",
            "  39:     x_gexpr = Dropout(0.1)(x_gexpr)\n",
            "  40:     x_gexpr = Dense((y_dim),activation='relu')(x_gexpr)\n",
            "  41:         #methylation feature\n",
            "  42:     x_methy = Dense(16)(methy_input)\n",
            "  43:     x_methy = Activation('tanh')(x_methy)\n",
            "  44:     x_methy = BatchNormalization()(x_methy)\n",
            "  45:     x_methy = Dropout(0.1)(x_methy)\n",
            "  46:     x_methy = Dense((y_dim),activation='relu')(x_methy)\n",
            "  47:     \n",
            "  48:     x = Concatenate()([x_mut,x_gexpr,x_methy])\n",
            "  49:     x = Dense(300,activation = 'relu')(x)\n",
            "  50:     x = Dropout(0.1)(x)\n",
            "  51:     x = Lambda(lambda x: K.expand_dims(x,axis=-1))(x)\n",
            "  52:     x = Lambda(lambda x: K.expand_dims(x,axis=1))(x)\n",
            "  53:     x = Conv2D(filters=30, kernel_size=(1,150),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
            "  54:     x = MaxPooling2D(pool_size=(1,2))(x)\n",
            "  55:     x = Conv2D(filters=10, kernel_size=(1,5),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
            "  56:     x = MaxPooling2D(pool_size=(1,3))(x)\n",
            "  57:     x = Conv2D(filters=5, kernel_size=(1,5),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
            "  58:     x = MaxPooling2D(pool_size=(1,3))(x)\n",
            "  59:     x = Dropout(0.1)(x)\n",
            "  60:     x = Flatten()(x)\n",
            "  61:     x = Dropout(0.1)(x)\n",
            "  62:     if regr:\n",
            "  63:       output = Dense((y_dim),name='output')(x)\n",
            "  64:     else:\n",
            "  65:       output = Dense(1,activation = 'sigmoid',name='output')(x)\n",
            "  66:     model  = Model(inputs=[mut_input,gexpr_input,methy_input],outputs=output)  \n",
            "  67:       \n",
            "  68:     model.compile(loss='mean_squared_error', metrics=['mse'],\n",
            "  69:                   optimizer=space['optimizer'])\n",
            "  70:     x_train=[X_train[0],X_train[1],X_train[2]]\n",
            "  71:     \n",
            "  72:     result = model.fit(x=x_train,y=Y_train,\n",
            "  73:               batch_size=space['batch_size'],\n",
            "  74:               epochs=300,\n",
            "  75:               verbose=2,\n",
            "  76:               validation_split=0.1)\n",
            "  77:     #get the highest validation accuracy of the training epochs\n",
            "  78:     x_test=[X_test[0],X_test[1],X_test[2]]\n",
            "  79:     print(\"RRRRRRRRRRRRRRRRRRR\")\n",
            "  80:     mse=model.evaluate([X_test[0],X_test[1],X_test[2]],Y_test, verbose=0)\n",
            "  81:     y_pred = model.predict([X_test[0],X_test[1],X_test[2]], verbose=0)\n",
            "  82:     DF1 = pd.DataFrame(y_pred)\n",
            "  83:     DF1.to_csv(\"pred_CDR.csv\")\n",
            "  84:     print(\"KKKKKKKKKKKKKKKKKKKKKKKKKK\")\n",
            "  85:     print(Y_test)\n",
            "  86:     DF2 = pd.DataFrame(Y_test)\n",
            "  87:     DF2.to_csv(\"real_CDR.csv\")\n",
            "  88:     #tr_acc = compute_accuracy(Y_test, y_pred)\n",
            "  89:     \n",
            "  90:     #print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
            "  91:    \n",
            "  92:     \n",
            "  93:     print(\"KKKKKKKKKKKKKKKKKKKKKKKKKK\")\n",
            "  94:     print(y_pred)\n",
            "  95:     print(\"KKKKKKKKKKKKKKKKKKKKKKKKKK\")\n",
            "  96:     print(Y_test)\n",
            "  97:     #overall_pcc = pearsonr(y_pred[:,0],Y_test)[0]\n",
            "  98:     #m=mean_squared_error(y_pred[:,0],Y_test)\n",
            "  99:     out = {\n",
            " 100:         \n",
            " 101:         'loss': mse[0],\n",
            " 102:         'status': STATUS_OK,\n",
            " 103:         'model_params': params,\n",
            " 104:     }\n",
            " 105:     #print(overall_pcc)\n",
            " 106:     print(\"KKKKKKKKKKKKKKKKKKKKKKKKKK\")\n",
            " 107:     temp_name = tempfile.gettempdir()+'/'+next(tempfile._get_candidate_names()) + '.h5'\n",
            " 108:     model.save(temp_name)\n",
            " 109:     with open(temp_name, 'rb') as infile:\n",
            " 110:         model_bytes = infile.read()\n",
            " 111:     out['model_serial'] = model_bytes\n",
            " 112:     print(\"i will be here %s\"%temp_name)\n",
            " 113:     return out\n",
            " 114: \n",
            "[ 0  1  2  4  5  6  7  8  9 10 11 12 13 15 16 17 18 20 21 22 23 24 25 26\n",
            " 28 29 30 31 33]\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 27\n",
            " 28 29 31 32 33]\n",
            "[ 0  1  3  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 23 24 25 26 27 28\n",
            " 29 30 31 32 33]\n",
            "[ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 19 20 21 22 24 25 26\n",
            " 27 30 31 32 33]\n",
            "[ 0  2  3  4  5  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
            " 28 29 30 31 32]\n",
            "[ 1  2  3  4  6  7  8  9 10 12 13 14 17 18 19 20 21 22 23 24 25 26 27 28\n",
            " 29 30 31 32 33]\n",
            "[ 0  1  2  3  4  5  6  7 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
            " 27 28 29 30 32 33]\n",
            "(30, 808)\n",
            "(30, 108)\n",
            "\r  0% 0/2 [00:00<?, ?it/s, best loss: ?]\r                                       \rAM BACK\n",
            "\r  0% 0/2 [00:00<?, ?it/s, best loss: ?]\r                                       \r26\n",
            "\r  0% 0/2 [00:00<?, ?it/s, best loss: ?]\r                                       \r108\n",
            "??????????????????????\n",
            "  0% 0/2 [00:00<?, ?it/s, best loss: ?]2021-12-28 13:31:34.370471: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Epoch 1/300\n",
            "1/1 - 2s - loss: 13.4572 - mse: 13.4572 - val_loss: 11.9607 - val_mse: 11.9607 - 2s/epoch - 2s/step\n",
            "\n",
            "Epoch 2/300\n",
            "1/1 - 0s - loss: 13.4361 - mse: 13.4361 - val_loss: 11.9486 - val_mse: 11.9486 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 3/300\n",
            "1/1 - 0s - loss: 13.4202 - mse: 13.4202 - val_loss: 11.9375 - val_mse: 11.9375 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 4/300\n",
            "1/1 - 0s - loss: 13.4008 - mse: 13.4008 - val_loss: 11.9272 - val_mse: 11.9272 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 5/300\n",
            "1/1 - 0s - loss: 13.3786 - mse: 13.3786 - val_loss: 11.9153 - val_mse: 11.9153 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 6/300\n",
            "1/1 - 0s - loss: 13.3567 - mse: 13.3567 - val_loss: 11.9019 - val_mse: 11.9019 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 7/300\n",
            "1/1 - 0s - loss: 13.3377 - mse: 13.3377 - val_loss: 11.8879 - val_mse: 11.8879 - 52ms/epoch - 52ms/step\n",
            "\n",
            "Epoch 8/300\n",
            "1/1 - 0s - loss: 13.3103 - mse: 13.3103 - val_loss: 11.8738 - val_mse: 11.8738 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 9/300\n",
            "1/1 - 0s - loss: 13.2898 - mse: 13.2898 - val_loss: 11.8593 - val_mse: 11.8593 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 10/300\n",
            "1/1 - 0s - loss: 13.2729 - mse: 13.2729 - val_loss: 11.8448 - val_mse: 11.8448 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 11/300\n",
            "1/1 - 0s - loss: 13.2390 - mse: 13.2390 - val_loss: 11.8295 - val_mse: 11.8295 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 12/300\n",
            "1/1 - 0s - loss: 13.2264 - mse: 13.2264 - val_loss: 11.8145 - val_mse: 11.8145 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 13/300\n",
            "1/1 - 0s - loss: 13.1961 - mse: 13.1961 - val_loss: 11.7978 - val_mse: 11.7978 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 14/300\n",
            "1/1 - 0s - loss: 13.1541 - mse: 13.1541 - val_loss: 11.7788 - val_mse: 11.7788 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 15/300\n",
            "1/1 - 0s - loss: 13.1168 - mse: 13.1168 - val_loss: 11.7580 - val_mse: 11.7580 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 16/300\n",
            "1/1 - 0s - loss: 13.1139 - mse: 13.1139 - val_loss: 11.7367 - val_mse: 11.7367 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 17/300\n",
            "1/1 - 0s - loss: 13.0394 - mse: 13.0394 - val_loss: 11.7117 - val_mse: 11.7117 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 18/300\n",
            "1/1 - 0s - loss: 13.0310 - mse: 13.0310 - val_loss: 11.6853 - val_mse: 11.6853 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 19/300\n",
            "1/1 - 0s - loss: 12.9825 - mse: 12.9825 - val_loss: 11.6562 - val_mse: 11.6562 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 20/300\n",
            "1/1 - 0s - loss: 12.8844 - mse: 12.8844 - val_loss: 11.6216 - val_mse: 11.6216 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 21/300\n",
            "1/1 - 0s - loss: 12.8618 - mse: 12.8618 - val_loss: 11.5839 - val_mse: 11.5839 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 22/300\n",
            "1/1 - 0s - loss: 12.7547 - mse: 12.7547 - val_loss: 11.5404 - val_mse: 11.5404 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 23/300\n",
            "1/1 - 0s - loss: 12.6543 - mse: 12.6543 - val_loss: 11.4887 - val_mse: 11.4887 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 24/300\n",
            "1/1 - 0s - loss: 12.6153 - mse: 12.6153 - val_loss: 11.4339 - val_mse: 11.4339 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 25/300\n",
            "1/1 - 0s - loss: 12.4957 - mse: 12.4957 - val_loss: 11.3709 - val_mse: 11.3709 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 26/300\n",
            "1/1 - 0s - loss: 12.4919 - mse: 12.4919 - val_loss: 11.3044 - val_mse: 11.3044 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 27/300\n",
            "1/1 - 0s - loss: 12.2896 - mse: 12.2896 - val_loss: 11.2206 - val_mse: 11.2206 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 28/300\n",
            "1/1 - 0s - loss: 12.2216 - mse: 12.2216 - val_loss: 11.1327 - val_mse: 11.1327 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 29/300\n",
            "1/1 - 0s - loss: 12.1335 - mse: 12.1335 - val_loss: 11.0393 - val_mse: 11.0393 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 30/300\n",
            "1/1 - 0s - loss: 11.7782 - mse: 11.7782 - val_loss: 10.9200 - val_mse: 10.9200 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 31/300\n",
            "1/1 - 0s - loss: 11.6024 - mse: 11.6024 - val_loss: 10.7825 - val_mse: 10.7825 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 32/300\n",
            "1/1 - 0s - loss: 11.3483 - mse: 11.3483 - val_loss: 10.6229 - val_mse: 10.6229 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 33/300\n",
            "1/1 - 0s - loss: 11.1580 - mse: 11.1580 - val_loss: 10.4575 - val_mse: 10.4575 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 34/300\n",
            "1/1 - 0s - loss: 10.7456 - mse: 10.7456 - val_loss: 10.2744 - val_mse: 10.2744 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 35/300\n",
            "1/1 - 0s - loss: 10.4528 - mse: 10.4528 - val_loss: 10.0619 - val_mse: 10.0619 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 36/300\n",
            "1/1 - 0s - loss: 10.2801 - mse: 10.2801 - val_loss: 9.8288 - val_mse: 9.8288 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 37/300\n",
            "1/1 - 0s - loss: 9.8814 - mse: 9.8814 - val_loss: 9.5792 - val_mse: 9.5792 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 38/300\n",
            "1/1 - 0s - loss: 9.4003 - mse: 9.4003 - val_loss: 9.3041 - val_mse: 9.3041 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 39/300\n",
            "1/1 - 0s - loss: 9.0825 - mse: 9.0825 - val_loss: 8.9971 - val_mse: 8.9971 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 40/300\n",
            "1/1 - 0s - loss: 8.6346 - mse: 8.6346 - val_loss: 8.7117 - val_mse: 8.7117 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 41/300\n",
            "1/1 - 0s - loss: 8.0147 - mse: 8.0147 - val_loss: 8.3833 - val_mse: 8.3833 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 42/300\n",
            "1/1 - 0s - loss: 7.8186 - mse: 7.8186 - val_loss: 8.0760 - val_mse: 8.0760 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 43/300\n",
            "1/1 - 0s - loss: 7.2715 - mse: 7.2715 - val_loss: 7.8234 - val_mse: 7.8234 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 44/300\n",
            "1/1 - 0s - loss: 7.3092 - mse: 7.3092 - val_loss: 7.5922 - val_mse: 7.5922 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 45/300\n",
            "1/1 - 0s - loss: 6.6126 - mse: 6.6126 - val_loss: 7.3371 - val_mse: 7.3371 - 57ms/epoch - 57ms/step\n",
            "\n",
            "Epoch 46/300\n",
            "1/1 - 0s - loss: 6.0792 - mse: 6.0792 - val_loss: 7.1117 - val_mse: 7.1117 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 47/300\n",
            "1/1 - 0s - loss: 5.9211 - mse: 5.9211 - val_loss: 6.8248 - val_mse: 6.8248 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 48/300\n",
            "1/1 - 0s - loss: 5.2884 - mse: 5.2884 - val_loss: 6.7252 - val_mse: 6.7252 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 49/300\n",
            "1/1 - 0s - loss: 5.3656 - mse: 5.3656 - val_loss: 6.4421 - val_mse: 6.4421 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 50/300\n",
            "1/1 - 0s - loss: 5.1947 - mse: 5.1947 - val_loss: 6.3054 - val_mse: 6.3054 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 51/300\n",
            "1/1 - 0s - loss: 4.4666 - mse: 4.4666 - val_loss: 6.1771 - val_mse: 6.1771 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 52/300\n",
            "1/1 - 0s - loss: 4.6527 - mse: 4.6527 - val_loss: 6.0759 - val_mse: 6.0759 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 53/300\n",
            "1/1 - 0s - loss: 4.2665 - mse: 4.2665 - val_loss: 5.9363 - val_mse: 5.9363 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 54/300\n",
            "1/1 - 0s - loss: 4.4258 - mse: 4.4258 - val_loss: 5.8569 - val_mse: 5.8569 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 55/300\n",
            "1/1 - 0s - loss: 4.1895 - mse: 4.1895 - val_loss: 5.7113 - val_mse: 5.7113 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 56/300\n",
            "1/1 - 0s - loss: 3.7463 - mse: 3.7463 - val_loss: 5.7162 - val_mse: 5.7162 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 57/300\n",
            "1/1 - 0s - loss: 3.5812 - mse: 3.5812 - val_loss: 5.6639 - val_mse: 5.6639 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 58/300\n",
            "1/1 - 0s - loss: 3.7746 - mse: 3.7746 - val_loss: 5.5650 - val_mse: 5.5650 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 59/300\n",
            "1/1 - 0s - loss: 3.6822 - mse: 3.6822 - val_loss: 5.3681 - val_mse: 5.3681 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 60/300\n",
            "1/1 - 0s - loss: 3.1449 - mse: 3.1449 - val_loss: 5.5431 - val_mse: 5.5431 - 36ms/epoch - 36ms/step\n",
            "\n",
            "Epoch 61/300\n",
            "1/1 - 0s - loss: 3.5781 - mse: 3.5781 - val_loss: 5.4107 - val_mse: 5.4107 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 62/300\n",
            "1/1 - 0s - loss: 3.3429 - mse: 3.3429 - val_loss: 5.3345 - val_mse: 5.3345 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 63/300\n",
            "1/1 - 0s - loss: 3.3366 - mse: 3.3366 - val_loss: 5.3474 - val_mse: 5.3474 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 64/300\n",
            "1/1 - 0s - loss: 3.0274 - mse: 3.0274 - val_loss: 5.3554 - val_mse: 5.3554 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 65/300\n",
            "1/1 - 0s - loss: 3.2523 - mse: 3.2523 - val_loss: 5.3030 - val_mse: 5.3030 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 66/300\n",
            "1/1 - 0s - loss: 2.9975 - mse: 2.9975 - val_loss: 5.3859 - val_mse: 5.3859 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 67/300\n",
            "1/1 - 0s - loss: 3.0327 - mse: 3.0327 - val_loss: 5.3642 - val_mse: 5.3642 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 68/300\n",
            "1/1 - 0s - loss: 3.1501 - mse: 3.1501 - val_loss: 5.2728 - val_mse: 5.2728 - 62ms/epoch - 62ms/step\n",
            "\n",
            "Epoch 69/300\n",
            "1/1 - 0s - loss: 2.9935 - mse: 2.9935 - val_loss: 5.2519 - val_mse: 5.2519 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 70/300\n",
            "1/1 - 0s - loss: 2.9446 - mse: 2.9446 - val_loss: 5.2203 - val_mse: 5.2203 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 71/300\n",
            "1/1 - 0s - loss: 2.9257 - mse: 2.9257 - val_loss: 5.1803 - val_mse: 5.1803 - 52ms/epoch - 52ms/step\n",
            "\n",
            "Epoch 72/300\n",
            "1/1 - 0s - loss: 3.0853 - mse: 3.0853 - val_loss: 5.1630 - val_mse: 5.1630 - 47ms/epoch - 47ms/step\n",
            "\n",
            "Epoch 73/300\n",
            "1/1 - 0s - loss: 2.7190 - mse: 2.7190 - val_loss: 5.1370 - val_mse: 5.1370 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 74/300\n",
            "1/1 - 0s - loss: 2.9747 - mse: 2.9747 - val_loss: 5.1074 - val_mse: 5.1074 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 75/300\n",
            "1/1 - 0s - loss: 2.7194 - mse: 2.7194 - val_loss: 5.0937 - val_mse: 5.0937 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 76/300\n",
            "1/1 - 0s - loss: 2.8744 - mse: 2.8744 - val_loss: 5.0746 - val_mse: 5.0746 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 77/300\n",
            "1/1 - 0s - loss: 2.6336 - mse: 2.6336 - val_loss: 5.0456 - val_mse: 5.0456 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 78/300\n",
            "1/1 - 0s - loss: 2.8176 - mse: 2.8176 - val_loss: 5.0462 - val_mse: 5.0462 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 79/300\n",
            "1/1 - 0s - loss: 2.7388 - mse: 2.7388 - val_loss: 5.0427 - val_mse: 5.0427 - 53ms/epoch - 53ms/step\n",
            "\n",
            "Epoch 80/300\n",
            "1/1 - 0s - loss: 2.7253 - mse: 2.7253 - val_loss: 5.0364 - val_mse: 5.0364 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 81/300\n",
            "1/1 - 0s - loss: 2.8372 - mse: 2.8372 - val_loss: 5.0669 - val_mse: 5.0669 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 82/300\n",
            "1/1 - 0s - loss: 2.8350 - mse: 2.8350 - val_loss: 5.0627 - val_mse: 5.0627 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 83/300\n",
            "1/1 - 0s - loss: 2.9029 - mse: 2.9029 - val_loss: 5.0724 - val_mse: 5.0724 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 84/300\n",
            "1/1 - 0s - loss: 2.5367 - mse: 2.5367 - val_loss: 5.1154 - val_mse: 5.1154 - 55ms/epoch - 55ms/step\n",
            "\n",
            "Epoch 85/300\n",
            "1/1 - 0s - loss: 3.0114 - mse: 3.0114 - val_loss: 5.0554 - val_mse: 5.0554 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 86/300\n",
            "1/1 - 0s - loss: 2.6958 - mse: 2.6958 - val_loss: 5.0333 - val_mse: 5.0333 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 87/300\n",
            "1/1 - 0s - loss: 2.7631 - mse: 2.7631 - val_loss: 5.0352 - val_mse: 5.0352 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 88/300\n",
            "1/1 - 0s - loss: 2.7220 - mse: 2.7220 - val_loss: 5.0922 - val_mse: 5.0922 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 89/300\n",
            "1/1 - 0s - loss: 2.8248 - mse: 2.8248 - val_loss: 5.0198 - val_mse: 5.0198 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 90/300\n",
            "1/1 - 0s - loss: 2.7317 - mse: 2.7317 - val_loss: 4.9427 - val_mse: 4.9427 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 91/300\n",
            "1/1 - 0s - loss: 2.5368 - mse: 2.5368 - val_loss: 5.0163 - val_mse: 5.0163 - 49ms/epoch - 49ms/step\n",
            "\n",
            "Epoch 92/300\n",
            "1/1 - 0s - loss: 2.6820 - mse: 2.6820 - val_loss: 5.0097 - val_mse: 5.0097 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 93/300\n",
            "1/1 - 0s - loss: 2.5286 - mse: 2.5286 - val_loss: 4.9066 - val_mse: 4.9066 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 94/300\n",
            "1/1 - 0s - loss: 2.7751 - mse: 2.7751 - val_loss: 4.7973 - val_mse: 4.7973 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 95/300\n",
            "1/1 - 0s - loss: 2.4803 - mse: 2.4803 - val_loss: 4.8025 - val_mse: 4.8025 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 96/300\n",
            "1/1 - 0s - loss: 2.6538 - mse: 2.6538 - val_loss: 4.9095 - val_mse: 4.9095 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 97/300\n",
            "1/1 - 0s - loss: 2.5792 - mse: 2.5792 - val_loss: 5.0001 - val_mse: 5.0001 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 98/300\n",
            "1/1 - 0s - loss: 2.6967 - mse: 2.6967 - val_loss: 4.9354 - val_mse: 4.9354 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 99/300\n",
            "1/1 - 0s - loss: 2.5868 - mse: 2.5868 - val_loss: 5.0108 - val_mse: 5.0108 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 100/300\n",
            "1/1 - 0s - loss: 2.5565 - mse: 2.5565 - val_loss: 5.0262 - val_mse: 5.0262 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 101/300\n",
            "1/1 - 0s - loss: 2.4360 - mse: 2.4360 - val_loss: 5.0773 - val_mse: 5.0773 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 102/300\n",
            "1/1 - 0s - loss: 2.5488 - mse: 2.5488 - val_loss: 5.0069 - val_mse: 5.0069 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 103/300\n",
            "1/1 - 0s - loss: 2.7204 - mse: 2.7204 - val_loss: 5.0144 - val_mse: 5.0144 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 104/300\n",
            "1/1 - 0s - loss: 2.5377 - mse: 2.5377 - val_loss: 5.1080 - val_mse: 5.1080 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 105/300\n",
            "1/1 - 0s - loss: 2.6370 - mse: 2.6370 - val_loss: 4.9849 - val_mse: 4.9849 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 106/300\n",
            "1/1 - 0s - loss: 2.5433 - mse: 2.5433 - val_loss: 5.0472 - val_mse: 5.0472 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 107/300\n",
            "1/1 - 0s - loss: 2.5925 - mse: 2.5925 - val_loss: 4.9987 - val_mse: 4.9987 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 108/300\n",
            "1/1 - 0s - loss: 2.6857 - mse: 2.6857 - val_loss: 4.9601 - val_mse: 4.9601 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 109/300\n",
            "1/1 - 0s - loss: 2.5343 - mse: 2.5343 - val_loss: 4.7541 - val_mse: 4.7541 - 63ms/epoch - 63ms/step\n",
            "\n",
            "Epoch 110/300\n",
            "1/1 - 0s - loss: 2.5105 - mse: 2.5105 - val_loss: 4.7577 - val_mse: 4.7577 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 111/300\n",
            "1/1 - 0s - loss: 2.4682 - mse: 2.4682 - val_loss: 4.7844 - val_mse: 4.7844 - 57ms/epoch - 57ms/step\n",
            "\n",
            "Epoch 112/300\n",
            "1/1 - 0s - loss: 2.5255 - mse: 2.5255 - val_loss: 4.7523 - val_mse: 4.7523 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 113/300\n",
            "1/1 - 0s - loss: 2.5116 - mse: 2.5116 - val_loss: 4.7575 - val_mse: 4.7575 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 114/300\n",
            "1/1 - 0s - loss: 2.4643 - mse: 2.4643 - val_loss: 4.8188 - val_mse: 4.8188 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 115/300\n",
            "1/1 - 0s - loss: 2.2673 - mse: 2.2673 - val_loss: 4.8088 - val_mse: 4.8088 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 116/300\n",
            "1/1 - 0s - loss: 2.4655 - mse: 2.4655 - val_loss: 4.7965 - val_mse: 4.7965 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 117/300\n",
            "1/1 - 0s - loss: 2.3683 - mse: 2.3683 - val_loss: 4.8143 - val_mse: 4.8143 - 57ms/epoch - 57ms/step\n",
            "\n",
            "Epoch 118/300\n",
            "1/1 - 0s - loss: 2.4722 - mse: 2.4722 - val_loss: 4.8963 - val_mse: 4.8963 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 119/300\n",
            "1/1 - 0s - loss: 2.4137 - mse: 2.4137 - val_loss: 4.9246 - val_mse: 4.9246 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 120/300\n",
            "1/1 - 0s - loss: 2.5088 - mse: 2.5088 - val_loss: 4.8577 - val_mse: 4.8577 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 121/300\n",
            "1/1 - 0s - loss: 2.6024 - mse: 2.6024 - val_loss: 4.7974 - val_mse: 4.7974 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 122/300\n",
            "1/1 - 0s - loss: 2.3369 - mse: 2.3369 - val_loss: 4.8785 - val_mse: 4.8785 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 123/300\n",
            "1/1 - 0s - loss: 2.3377 - mse: 2.3377 - val_loss: 4.8735 - val_mse: 4.8735 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 124/300\n",
            "1/1 - 0s - loss: 2.4291 - mse: 2.4291 - val_loss: 4.8937 - val_mse: 4.8937 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 125/300\n",
            "1/1 - 0s - loss: 2.5252 - mse: 2.5252 - val_loss: 4.8401 - val_mse: 4.8401 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 126/300\n",
            "1/1 - 0s - loss: 2.5035 - mse: 2.5035 - val_loss: 4.8925 - val_mse: 4.8925 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 127/300\n",
            "1/1 - 0s - loss: 2.6716 - mse: 2.6716 - val_loss: 4.8598 - val_mse: 4.8598 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 128/300\n",
            "1/1 - 0s - loss: 2.5330 - mse: 2.5330 - val_loss: 4.8681 - val_mse: 4.8681 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 129/300\n",
            "1/1 - 0s - loss: 2.7281 - mse: 2.7281 - val_loss: 4.8263 - val_mse: 4.8263 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 130/300\n",
            "1/1 - 0s - loss: 2.4244 - mse: 2.4244 - val_loss: 4.8481 - val_mse: 4.8481 - 36ms/epoch - 36ms/step\n",
            "\n",
            "Epoch 131/300\n",
            "1/1 - 0s - loss: 2.5080 - mse: 2.5080 - val_loss: 4.8573 - val_mse: 4.8573 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 132/300\n",
            "1/1 - 0s - loss: 2.5737 - mse: 2.5737 - val_loss: 4.8290 - val_mse: 4.8290 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 133/300\n",
            "1/1 - 0s - loss: 2.8026 - mse: 2.8026 - val_loss: 4.8830 - val_mse: 4.8830 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 134/300\n",
            "1/1 - 0s - loss: 2.4805 - mse: 2.4805 - val_loss: 4.8815 - val_mse: 4.8815 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 135/300\n",
            "1/1 - 0s - loss: 2.4843 - mse: 2.4843 - val_loss: 4.8578 - val_mse: 4.8578 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 136/300\n",
            "1/1 - 0s - loss: 2.5032 - mse: 2.5032 - val_loss: 4.8405 - val_mse: 4.8405 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 137/300\n",
            "1/1 - 0s - loss: 2.7192 - mse: 2.7192 - val_loss: 4.8198 - val_mse: 4.8198 - 36ms/epoch - 36ms/step\n",
            "\n",
            "Epoch 138/300\n",
            "1/1 - 0s - loss: 2.4757 - mse: 2.4757 - val_loss: 4.7948 - val_mse: 4.7948 - 36ms/epoch - 36ms/step\n",
            "\n",
            "Epoch 139/300\n",
            "1/1 - 0s - loss: 2.4679 - mse: 2.4679 - val_loss: 4.8791 - val_mse: 4.8791 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 140/300\n",
            "1/1 - 0s - loss: 2.2773 - mse: 2.2773 - val_loss: 4.8682 - val_mse: 4.8682 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 141/300\n",
            "1/1 - 0s - loss: 2.5845 - mse: 2.5845 - val_loss: 4.7845 - val_mse: 4.7845 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 142/300\n",
            "1/1 - 0s - loss: 2.4427 - mse: 2.4427 - val_loss: 4.7878 - val_mse: 4.7878 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 143/300\n",
            "1/1 - 0s - loss: 2.4789 - mse: 2.4789 - val_loss: 4.8035 - val_mse: 4.8035 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 144/300\n",
            "1/1 - 0s - loss: 2.3254 - mse: 2.3254 - val_loss: 4.7793 - val_mse: 4.7793 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 145/300\n",
            "1/1 - 0s - loss: 2.3966 - mse: 2.3966 - val_loss: 4.7573 - val_mse: 4.7573 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 146/300\n",
            "1/1 - 0s - loss: 2.3220 - mse: 2.3220 - val_loss: 4.7489 - val_mse: 4.7489 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 147/300\n",
            "1/1 - 0s - loss: 2.3186 - mse: 2.3186 - val_loss: 4.7328 - val_mse: 4.7328 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 148/300\n",
            "1/1 - 0s - loss: 2.4945 - mse: 2.4945 - val_loss: 4.7508 - val_mse: 4.7508 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 149/300\n",
            "1/1 - 0s - loss: 2.3804 - mse: 2.3804 - val_loss: 4.7704 - val_mse: 4.7704 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 150/300\n",
            "1/1 - 0s - loss: 2.5401 - mse: 2.5401 - val_loss: 4.7929 - val_mse: 4.7929 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 151/300\n",
            "1/1 - 0s - loss: 2.2670 - mse: 2.2670 - val_loss: 4.7582 - val_mse: 4.7582 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 152/300\n",
            "1/1 - 0s - loss: 2.4247 - mse: 2.4247 - val_loss: 4.8159 - val_mse: 4.8159 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 153/300\n",
            "1/1 - 0s - loss: 2.3741 - mse: 2.3741 - val_loss: 4.8020 - val_mse: 4.8020 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 154/300\n",
            "1/1 - 0s - loss: 2.3597 - mse: 2.3597 - val_loss: 4.8424 - val_mse: 4.8424 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 155/300\n",
            "1/1 - 0s - loss: 2.3839 - mse: 2.3839 - val_loss: 4.8136 - val_mse: 4.8136 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 156/300\n",
            "1/1 - 0s - loss: 2.6215 - mse: 2.6215 - val_loss: 4.8217 - val_mse: 4.8217 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 157/300\n",
            "1/1 - 0s - loss: 2.3117 - mse: 2.3117 - val_loss: 4.8576 - val_mse: 4.8576 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 158/300\n",
            "1/1 - 0s - loss: 2.4306 - mse: 2.4306 - val_loss: 4.8187 - val_mse: 4.8187 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 159/300\n",
            "1/1 - 0s - loss: 2.3654 - mse: 2.3654 - val_loss: 4.7824 - val_mse: 4.7824 - 56ms/epoch - 56ms/step\n",
            "\n",
            "Epoch 160/300\n",
            "1/1 - 0s - loss: 2.4264 - mse: 2.4264 - val_loss: 4.7700 - val_mse: 4.7700 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 161/300\n",
            "1/1 - 0s - loss: 2.6881 - mse: 2.6881 - val_loss: 4.7357 - val_mse: 4.7357 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 162/300\n",
            "1/1 - 0s - loss: 2.3435 - mse: 2.3435 - val_loss: 4.7953 - val_mse: 4.7953 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 163/300\n",
            "1/1 - 0s - loss: 2.6175 - mse: 2.6175 - val_loss: 4.7719 - val_mse: 4.7719 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 164/300\n",
            "1/1 - 0s - loss: 2.5209 - mse: 2.5209 - val_loss: 4.8309 - val_mse: 4.8309 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 165/300\n",
            "1/1 - 0s - loss: 2.3728 - mse: 2.3728 - val_loss: 4.7952 - val_mse: 4.7952 - 51ms/epoch - 51ms/step\n",
            "\n",
            "Epoch 166/300\n",
            "1/1 - 0s - loss: 2.4286 - mse: 2.4286 - val_loss: 4.7743 - val_mse: 4.7743 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 167/300\n",
            "1/1 - 0s - loss: 2.2028 - mse: 2.2028 - val_loss: 4.7890 - val_mse: 4.7890 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 168/300\n",
            "1/1 - 0s - loss: 2.3346 - mse: 2.3346 - val_loss: 4.7745 - val_mse: 4.7745 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 169/300\n",
            "1/1 - 0s - loss: 2.2240 - mse: 2.2240 - val_loss: 4.7684 - val_mse: 4.7684 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 170/300\n",
            "1/1 - 0s - loss: 2.2593 - mse: 2.2593 - val_loss: 4.7378 - val_mse: 4.7378 - 36ms/epoch - 36ms/step\n",
            "\n",
            "Epoch 171/300\n",
            "1/1 - 0s - loss: 2.2769 - mse: 2.2769 - val_loss: 4.7113 - val_mse: 4.7113 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 172/300\n",
            "1/1 - 0s - loss: 2.1914 - mse: 2.1914 - val_loss: 4.7040 - val_mse: 4.7040 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 173/300\n",
            "1/1 - 0s - loss: 2.2229 - mse: 2.2229 - val_loss: 4.7019 - val_mse: 4.7019 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 174/300\n",
            "1/1 - 0s - loss: 2.4223 - mse: 2.4223 - val_loss: 4.7081 - val_mse: 4.7081 - 36ms/epoch - 36ms/step\n",
            "\n",
            "Epoch 175/300\n",
            "1/1 - 0s - loss: 2.4422 - mse: 2.4422 - val_loss: 4.7207 - val_mse: 4.7207 - 36ms/epoch - 36ms/step\n",
            "\n",
            "Epoch 176/300\n",
            "1/1 - 0s - loss: 2.4700 - mse: 2.4700 - val_loss: 4.7221 - val_mse: 4.7221 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 177/300\n",
            "1/1 - 0s - loss: 2.2264 - mse: 2.2264 - val_loss: 4.7542 - val_mse: 4.7542 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 178/300\n",
            "1/1 - 0s - loss: 2.4816 - mse: 2.4816 - val_loss: 4.7133 - val_mse: 4.7133 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 179/300\n",
            "1/1 - 0s - loss: 2.3342 - mse: 2.3342 - val_loss: 4.7203 - val_mse: 4.7203 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 180/300\n",
            "1/1 - 0s - loss: 2.3683 - mse: 2.3683 - val_loss: 4.7349 - val_mse: 4.7349 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 181/300\n",
            "1/1 - 0s - loss: 2.5731 - mse: 2.5731 - val_loss: 4.7176 - val_mse: 4.7176 - 36ms/epoch - 36ms/step\n",
            "\n",
            "Epoch 182/300\n",
            "1/1 - 0s - loss: 2.2394 - mse: 2.2394 - val_loss: 4.7074 - val_mse: 4.7074 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 183/300\n",
            "1/1 - 0s - loss: 2.1634 - mse: 2.1634 - val_loss: 4.6803 - val_mse: 4.6803 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 184/300\n",
            "1/1 - 0s - loss: 2.3513 - mse: 2.3513 - val_loss: 4.6980 - val_mse: 4.6980 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 185/300\n",
            "1/1 - 0s - loss: 2.2652 - mse: 2.2652 - val_loss: 4.6779 - val_mse: 4.6779 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 186/300\n",
            "1/1 - 0s - loss: 2.3627 - mse: 2.3627 - val_loss: 4.6800 - val_mse: 4.6800 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 187/300\n",
            "1/1 - 0s - loss: 2.1944 - mse: 2.1944 - val_loss: 4.6734 - val_mse: 4.6734 - 36ms/epoch - 36ms/step\n",
            "\n",
            "Epoch 188/300\n",
            "1/1 - 0s - loss: 2.3913 - mse: 2.3913 - val_loss: 4.7054 - val_mse: 4.7054 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 189/300\n",
            "1/1 - 0s - loss: 2.1157 - mse: 2.1157 - val_loss: 4.6929 - val_mse: 4.6929 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 190/300\n",
            "1/1 - 0s - loss: 2.3285 - mse: 2.3285 - val_loss: 4.6988 - val_mse: 4.6988 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 191/300\n",
            "1/1 - 0s - loss: 2.3116 - mse: 2.3116 - val_loss: 4.6747 - val_mse: 4.6747 - 67ms/epoch - 67ms/step\n",
            "\n",
            "Epoch 192/300\n",
            "1/1 - 0s - loss: 2.2692 - mse: 2.2692 - val_loss: 4.6729 - val_mse: 4.6729 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 193/300\n",
            "1/1 - 0s - loss: 2.4599 - mse: 2.4599 - val_loss: 4.6862 - val_mse: 4.6862 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 194/300\n",
            "1/1 - 0s - loss: 2.3522 - mse: 2.3522 - val_loss: 4.7034 - val_mse: 4.7034 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 195/300\n",
            "1/1 - 0s - loss: 2.2505 - mse: 2.2505 - val_loss: 4.6613 - val_mse: 4.6613 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 196/300\n",
            "1/1 - 0s - loss: 2.3998 - mse: 2.3998 - val_loss: 4.6736 - val_mse: 4.6736 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 197/300\n",
            "1/1 - 0s - loss: 2.4315 - mse: 2.4315 - val_loss: 4.7291 - val_mse: 4.7291 - 36ms/epoch - 36ms/step\n",
            "\n",
            "Epoch 198/300\n",
            "1/1 - 0s - loss: 2.3333 - mse: 2.3333 - val_loss: 4.6835 - val_mse: 4.6835 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 199/300\n",
            "1/1 - 0s - loss: 2.3671 - mse: 2.3671 - val_loss: 4.6800 - val_mse: 4.6800 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 200/300\n",
            "1/1 - 0s - loss: 2.1860 - mse: 2.1860 - val_loss: 4.7038 - val_mse: 4.7038 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 201/300\n",
            "1/1 - 0s - loss: 2.3336 - mse: 2.3336 - val_loss: 4.7851 - val_mse: 4.7851 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 202/300\n",
            "1/1 - 0s - loss: 2.1856 - mse: 2.1856 - val_loss: 4.8100 - val_mse: 4.8100 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 203/300\n",
            "1/1 - 0s - loss: 2.3106 - mse: 2.3106 - val_loss: 4.8603 - val_mse: 4.8603 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 204/300\n",
            "1/1 - 0s - loss: 2.3897 - mse: 2.3897 - val_loss: 4.7424 - val_mse: 4.7424 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 205/300\n",
            "1/1 - 0s - loss: 2.1742 - mse: 2.1742 - val_loss: 4.7725 - val_mse: 4.7725 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 206/300\n",
            "1/1 - 0s - loss: 2.2333 - mse: 2.2333 - val_loss: 4.7699 - val_mse: 4.7699 - 47ms/epoch - 47ms/step\n",
            "\n",
            "Epoch 207/300\n",
            "1/1 - 0s - loss: 2.2300 - mse: 2.2300 - val_loss: 4.7751 - val_mse: 4.7751 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 208/300\n",
            "1/1 - 0s - loss: 2.1693 - mse: 2.1693 - val_loss: 4.7122 - val_mse: 4.7122 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 209/300\n",
            "1/1 - 0s - loss: 2.1659 - mse: 2.1659 - val_loss: 4.7744 - val_mse: 4.7744 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 210/300\n",
            "1/1 - 0s - loss: 2.0695 - mse: 2.0695 - val_loss: 4.7447 - val_mse: 4.7447 - 56ms/epoch - 56ms/step\n",
            "\n",
            "Epoch 211/300\n",
            "1/1 - 0s - loss: 2.1187 - mse: 2.1187 - val_loss: 4.7262 - val_mse: 4.7262 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 212/300\n",
            "1/1 - 0s - loss: 2.2877 - mse: 2.2877 - val_loss: 4.8044 - val_mse: 4.8044 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 213/300\n",
            "1/1 - 0s - loss: 2.2915 - mse: 2.2915 - val_loss: 4.7642 - val_mse: 4.7642 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 214/300\n",
            "1/1 - 0s - loss: 2.3746 - mse: 2.3746 - val_loss: 4.7890 - val_mse: 4.7890 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 215/300\n",
            "1/1 - 0s - loss: 2.1282 - mse: 2.1282 - val_loss: 4.7245 - val_mse: 4.7245 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 216/300\n",
            "1/1 - 0s - loss: 2.1574 - mse: 2.1574 - val_loss: 4.6834 - val_mse: 4.6834 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 217/300\n",
            "1/1 - 0s - loss: 2.2665 - mse: 2.2665 - val_loss: 4.6821 - val_mse: 4.6821 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 218/300\n",
            "1/1 - 0s - loss: 2.2403 - mse: 2.2403 - val_loss: 4.6949 - val_mse: 4.6949 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 219/300\n",
            "1/1 - 0s - loss: 2.3302 - mse: 2.3302 - val_loss: 4.7035 - val_mse: 4.7035 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 220/300\n",
            "1/1 - 0s - loss: 2.1636 - mse: 2.1636 - val_loss: 4.6719 - val_mse: 4.6719 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 221/300\n",
            "1/1 - 0s - loss: 2.3258 - mse: 2.3258 - val_loss: 4.7185 - val_mse: 4.7185 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 222/300\n",
            "1/1 - 0s - loss: 2.2134 - mse: 2.2134 - val_loss: 4.6930 - val_mse: 4.6930 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 223/300\n",
            "1/1 - 0s - loss: 2.2753 - mse: 2.2753 - val_loss: 4.6990 - val_mse: 4.6990 - 57ms/epoch - 57ms/step\n",
            "\n",
            "Epoch 224/300\n",
            "1/1 - 0s - loss: 2.2969 - mse: 2.2969 - val_loss: 4.7042 - val_mse: 4.7042 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 225/300\n",
            "1/1 - 0s - loss: 2.2595 - mse: 2.2595 - val_loss: 4.8910 - val_mse: 4.8910 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 226/300\n",
            "1/1 - 0s - loss: 2.3864 - mse: 2.3864 - val_loss: 4.9697 - val_mse: 4.9697 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 227/300\n",
            "1/1 - 0s - loss: 2.1898 - mse: 2.1898 - val_loss: 4.7549 - val_mse: 4.7549 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 228/300\n",
            "1/1 - 0s - loss: 2.4125 - mse: 2.4125 - val_loss: 4.9086 - val_mse: 4.9086 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 229/300\n",
            "1/1 - 0s - loss: 2.3018 - mse: 2.3018 - val_loss: 4.9226 - val_mse: 4.9226 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 230/300\n",
            "1/1 - 0s - loss: 2.1619 - mse: 2.1619 - val_loss: 4.9395 - val_mse: 4.9395 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 231/300\n",
            "1/1 - 0s - loss: 2.2453 - mse: 2.2453 - val_loss: 4.9633 - val_mse: 4.9633 - 49ms/epoch - 49ms/step\n",
            "\n",
            "Epoch 232/300\n",
            "1/1 - 0s - loss: 2.2501 - mse: 2.2501 - val_loss: 5.0067 - val_mse: 5.0067 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 233/300\n",
            "1/1 - 0s - loss: 2.1952 - mse: 2.1952 - val_loss: 4.9585 - val_mse: 4.9585 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 234/300\n",
            "1/1 - 0s - loss: 2.2000 - mse: 2.2000 - val_loss: 4.9892 - val_mse: 4.9892 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 235/300\n",
            "1/1 - 0s - loss: 2.1378 - mse: 2.1378 - val_loss: 5.0162 - val_mse: 5.0162 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 236/300\n",
            "1/1 - 0s - loss: 2.1960 - mse: 2.1960 - val_loss: 4.8933 - val_mse: 4.8933 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 237/300\n",
            "1/1 - 0s - loss: 2.2235 - mse: 2.2235 - val_loss: 5.0825 - val_mse: 5.0825 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 238/300\n",
            "1/1 - 0s - loss: 2.2716 - mse: 2.2716 - val_loss: 5.1540 - val_mse: 5.1540 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 239/300\n",
            "1/1 - 0s - loss: 2.1641 - mse: 2.1641 - val_loss: 5.0142 - val_mse: 5.0142 - 55ms/epoch - 55ms/step\n",
            "\n",
            "Epoch 240/300\n",
            "1/1 - 0s - loss: 2.1394 - mse: 2.1394 - val_loss: 5.1199 - val_mse: 5.1199 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 241/300\n",
            "1/1 - 0s - loss: 2.0605 - mse: 2.0605 - val_loss: 4.9293 - val_mse: 4.9293 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 242/300\n",
            "1/1 - 0s - loss: 2.2585 - mse: 2.2585 - val_loss: 5.0117 - val_mse: 5.0117 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 243/300\n",
            "1/1 - 0s - loss: 2.1917 - mse: 2.1917 - val_loss: 5.0956 - val_mse: 5.0956 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 244/300\n",
            "1/1 - 0s - loss: 2.3402 - mse: 2.3402 - val_loss: 5.1186 - val_mse: 5.1186 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 245/300\n",
            "1/1 - 0s - loss: 2.1803 - mse: 2.1803 - val_loss: 5.1707 - val_mse: 5.1707 - 47ms/epoch - 47ms/step\n",
            "\n",
            "Epoch 246/300\n",
            "1/1 - 0s - loss: 2.1050 - mse: 2.1050 - val_loss: 5.0198 - val_mse: 5.0198 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 247/300\n",
            "1/1 - 0s - loss: 2.1727 - mse: 2.1727 - val_loss: 5.1517 - val_mse: 5.1517 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 248/300\n",
            "1/1 - 0s - loss: 2.1894 - mse: 2.1894 - val_loss: 5.0487 - val_mse: 5.0487 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 249/300\n",
            "1/1 - 0s - loss: 2.0241 - mse: 2.0241 - val_loss: 5.0596 - val_mse: 5.0596 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 250/300\n",
            "1/1 - 0s - loss: 2.1295 - mse: 2.1295 - val_loss: 5.1027 - val_mse: 5.1027 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 251/300\n",
            "1/1 - 0s - loss: 2.2105 - mse: 2.2105 - val_loss: 5.1576 - val_mse: 5.1576 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 252/300\n",
            "1/1 - 0s - loss: 2.0828 - mse: 2.0828 - val_loss: 5.0695 - val_mse: 5.0695 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 253/300\n",
            "1/1 - 0s - loss: 2.2504 - mse: 2.2504 - val_loss: 5.1321 - val_mse: 5.1321 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 254/300\n",
            "1/1 - 0s - loss: 2.1786 - mse: 2.1786 - val_loss: 4.9887 - val_mse: 4.9887 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 255/300\n",
            "1/1 - 0s - loss: 2.1217 - mse: 2.1217 - val_loss: 5.0948 - val_mse: 5.0948 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 256/300\n",
            "1/1 - 0s - loss: 2.1450 - mse: 2.1450 - val_loss: 4.9425 - val_mse: 4.9425 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 257/300\n",
            "1/1 - 0s - loss: 2.2199 - mse: 2.2199 - val_loss: 5.1953 - val_mse: 5.1953 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 258/300\n",
            "1/1 - 0s - loss: 2.1618 - mse: 2.1618 - val_loss: 4.9503 - val_mse: 4.9503 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 259/300\n",
            "1/1 - 0s - loss: 2.2252 - mse: 2.2252 - val_loss: 5.1869 - val_mse: 5.1869 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 260/300\n",
            "1/1 - 0s - loss: 2.2876 - mse: 2.2876 - val_loss: 5.0243 - val_mse: 5.0243 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 261/300\n",
            "1/1 - 0s - loss: 2.2201 - mse: 2.2201 - val_loss: 5.0287 - val_mse: 5.0287 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 262/300\n",
            "1/1 - 0s - loss: 2.1471 - mse: 2.1471 - val_loss: 5.0318 - val_mse: 5.0318 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 263/300\n",
            "1/1 - 0s - loss: 2.1098 - mse: 2.1098 - val_loss: 5.0866 - val_mse: 5.0866 - 54ms/epoch - 54ms/step\n",
            "\n",
            "Epoch 264/300\n",
            "1/1 - 0s - loss: 2.1708 - mse: 2.1708 - val_loss: 4.9944 - val_mse: 4.9944 - 53ms/epoch - 53ms/step\n",
            "\n",
            "Epoch 265/300\n",
            "1/1 - 0s - loss: 2.2712 - mse: 2.2712 - val_loss: 5.0943 - val_mse: 5.0943 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 266/300\n",
            "1/1 - 0s - loss: 2.2125 - mse: 2.2125 - val_loss: 5.1315 - val_mse: 5.1315 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 267/300\n",
            "1/1 - 0s - loss: 2.1540 - mse: 2.1540 - val_loss: 5.0637 - val_mse: 5.0637 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 268/300\n",
            "1/1 - 0s - loss: 2.0962 - mse: 2.0962 - val_loss: 4.9479 - val_mse: 4.9479 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 269/300\n",
            "1/1 - 0s - loss: 2.2705 - mse: 2.2705 - val_loss: 5.1570 - val_mse: 5.1570 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 270/300\n",
            "1/1 - 0s - loss: 2.0713 - mse: 2.0713 - val_loss: 5.1034 - val_mse: 5.1034 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 271/300\n",
            "1/1 - 0s - loss: 2.1998 - mse: 2.1998 - val_loss: 5.1816 - val_mse: 5.1816 - 47ms/epoch - 47ms/step\n",
            "\n",
            "Epoch 272/300\n",
            "1/1 - 0s - loss: 2.1082 - mse: 2.1082 - val_loss: 5.1524 - val_mse: 5.1524 - 47ms/epoch - 47ms/step\n",
            "\n",
            "Epoch 273/300\n",
            "1/1 - 0s - loss: 2.1631 - mse: 2.1631 - val_loss: 5.0054 - val_mse: 5.0054 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 274/300\n",
            "1/1 - 0s - loss: 2.3168 - mse: 2.3168 - val_loss: 5.2165 - val_mse: 5.2165 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 275/300\n",
            "1/1 - 0s - loss: 2.0472 - mse: 2.0472 - val_loss: 4.9173 - val_mse: 4.9173 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 276/300\n",
            "1/1 - 0s - loss: 2.2540 - mse: 2.2540 - val_loss: 5.0580 - val_mse: 5.0580 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 277/300\n",
            "1/1 - 0s - loss: 2.1826 - mse: 2.1826 - val_loss: 5.1627 - val_mse: 5.1627 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 278/300\n",
            "1/1 - 0s - loss: 2.0493 - mse: 2.0493 - val_loss: 4.9893 - val_mse: 4.9893 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 279/300\n",
            "1/1 - 0s - loss: 2.1932 - mse: 2.1932 - val_loss: 5.1413 - val_mse: 5.1413 - 56ms/epoch - 56ms/step\n",
            "\n",
            "Epoch 280/300\n",
            "1/1 - 0s - loss: 2.3712 - mse: 2.3712 - val_loss: 5.1570 - val_mse: 5.1570 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 281/300\n",
            "1/1 - 0s - loss: 2.2947 - mse: 2.2947 - val_loss: 5.0639 - val_mse: 5.0639 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 282/300\n",
            "1/1 - 0s - loss: 2.0866 - mse: 2.0866 - val_loss: 5.2409 - val_mse: 5.2409 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 283/300\n",
            "1/1 - 0s - loss: 2.2035 - mse: 2.2035 - val_loss: 4.9717 - val_mse: 4.9717 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 284/300\n",
            "1/1 - 0s - loss: 2.1284 - mse: 2.1284 - val_loss: 5.0941 - val_mse: 5.0941 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 285/300\n",
            "1/1 - 0s - loss: 2.0696 - mse: 2.0696 - val_loss: 5.0297 - val_mse: 5.0297 - 55ms/epoch - 55ms/step\n",
            "\n",
            "Epoch 286/300\n",
            "1/1 - 0s - loss: 2.1930 - mse: 2.1930 - val_loss: 4.9645 - val_mse: 4.9645 - 47ms/epoch - 47ms/step\n",
            "\n",
            "Epoch 287/300\n",
            "1/1 - 0s - loss: 2.3232 - mse: 2.3232 - val_loss: 5.2480 - val_mse: 5.2480 - 56ms/epoch - 56ms/step\n",
            "\n",
            "Epoch 288/300\n",
            "1/1 - 0s - loss: 2.1415 - mse: 2.1415 - val_loss: 5.0731 - val_mse: 5.0731 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 289/300\n",
            "1/1 - 0s - loss: 2.1453 - mse: 2.1453 - val_loss: 5.0580 - val_mse: 5.0580 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 290/300\n",
            "1/1 - 0s - loss: 2.0980 - mse: 2.0980 - val_loss: 5.0320 - val_mse: 5.0320 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 291/300\n",
            "1/1 - 0s - loss: 2.1679 - mse: 2.1679 - val_loss: 5.0077 - val_mse: 5.0077 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 292/300\n",
            "1/1 - 0s - loss: 2.3085 - mse: 2.3085 - val_loss: 5.0946 - val_mse: 5.0946 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 293/300\n",
            "1/1 - 0s - loss: 2.0437 - mse: 2.0437 - val_loss: 5.1186 - val_mse: 5.1186 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 294/300\n",
            "1/1 - 0s - loss: 1.9702 - mse: 1.9702 - val_loss: 4.8094 - val_mse: 4.8094 - 37ms/epoch - 37ms/step\n",
            "\n",
            "Epoch 295/300\n",
            "1/1 - 0s - loss: 2.2654 - mse: 2.2654 - val_loss: 5.1222 - val_mse: 5.1222 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 296/300\n",
            "1/1 - 0s - loss: 2.2590 - mse: 2.2590 - val_loss: 5.1500 - val_mse: 5.1500 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 297/300\n",
            "1/1 - 0s - loss: 2.1932 - mse: 2.1932 - val_loss: 5.1057 - val_mse: 5.1057 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 298/300\n",
            "1/1 - 0s - loss: 2.0911 - mse: 2.0911 - val_loss: 4.9564 - val_mse: 4.9564 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 299/300\n",
            "1/1 - 0s - loss: 2.0067 - mse: 2.0067 - val_loss: 4.9001 - val_mse: 4.9001 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 300/300\n",
            "1/1 - 0s - loss: 2.0929 - mse: 2.0929 - val_loss: 5.1081 - val_mse: 5.1081 - 39ms/epoch - 39ms/step\n",
            "\n",
            "RRRRRRRRRRRRRRRRRRR\n",
            "KKKKKKKKKKKKKKKKKKKKKKKKKK\n",
            "[[ 3.14200123 -4.24168481  3.50820838  3.45301098 -0.82878683  3.55198812\n",
            "  -2.39865244 -6.12672762 -0.10672683  0.71976341 -3.14990379  0.88687742\n",
            "   1.96307042 -2.96602867 -0.96485306  3.0573246   1.80836617  0.69483106\n",
            "  -3.59916629  0.88428494 -4.14375598  1.32681998  1.15791418  3.10145464\n",
            "  -3.94394818 -0.45704923 -3.17973647  2.01345678  3.49290798  1.08630887\n",
            "   1.91519355  1.99006189 -0.89089204  3.10862606  0.31465888  1.96238638\n",
            "   1.18530034  3.04548214  0.68146498  3.21140606  3.70074931 -0.6222812\n",
            "   0.33585268 -0.49023033  2.30214294 -1.62807221 -4.09654536  2.57267535\n",
            "   4.62704694  3.50130149 -4.55075547  2.32779749 -3.0686732   2.30381479\n",
            "   2.52119496  3.81951609  3.41332524  3.67610632 -6.74182502 -5.01156443\n",
            "   1.5462474  -1.05207687  1.35996932  2.44520436  0.37169979 -0.35820057\n",
            "  -5.11575173 -0.6314072   0.32288053  1.46943856  2.76690953  3.37200363\n",
            "   1.7394532  -6.58334416  3.5319249   1.6127086   3.13503207 -0.66395155\n",
            "   4.06999663  1.15799549  4.73989636  4.31418599  1.44526233  0.28127454\n",
            "   2.64671056 -4.54198503  3.74632638 -3.03609444  1.52410275 -3.03882508\n",
            "  -2.73518963  1.30597321  2.11105062  2.67127132  3.57314248 -1.12735046\n",
            "  -1.6723929  -0.46496717  0.65868609  4.05736274  0.85526518 -2.72253459\n",
            "   3.54003411  5.17118197  5.18668262  3.66831296  3.82868818  1.1772571 ]\n",
            " [ 3.40599731  5.7134509   5.25723885  5.24045097  4.74172481  4.90486413\n",
            "   5.09260533 -5.02544379  5.72105329  2.18549181 -3.21533712  1.64194021\n",
            "   2.90787944  0.80145692 -0.59077416  5.30981217  2.28761019  0.42374435\n",
            "   2.7811058   0.78562168 -3.47333691  2.90658546  2.36947666  6.32784573\n",
            "   2.86011799  4.23956907  0.03411955  3.819755    4.24397878  2.2907991\n",
            "   0.21054544  3.32214512 -1.90970399  4.45317078  0.6683813   1.59365058\n",
            "   1.68440314  5.11113288  2.99692001  5.99923782  6.64309868  5.40295846\n",
            "   0.32490177  0.69211005  3.29979136 -1.60424629 -6.62905308  4.90352694\n",
            "   4.99688408  4.12495816 -2.9216337   3.43068149  2.60198514  5.95873753\n",
            "   2.83167517  4.34509765  3.82192717  6.32176238  0.12217197  2.80634905\n",
            "  -0.86734911 -0.21511189  3.53131945  2.87442117 -1.92782223  0.3331608\n",
            "  -4.27693843  0.93958593  1.16395248  3.26064057  3.99138537  5.40150642\n",
            "   4.2357398   2.43908912  5.01012806  3.44314736  5.89294117  1.61704812\n",
            "   4.24549075 -1.43617676  6.00028627  6.64315603  4.89968855  5.1618463\n",
            "   2.60143546 -2.46206637  2.71564655 -3.14757091  0.1206718  -2.06879665\n",
            "  -5.83264703  1.05089459  1.72550644  2.60879083  2.79398982  0.15725611\n",
            "  -1.78106163  1.14279554  1.38952026  3.71189082  0.87435186 -3.04563706\n",
            "   0.97094974  5.29104355  4.50302929  3.90080892  3.59288026  2.2498049 ]\n",
            " [ 1.96404004  4.18793665  3.72307462  3.83831099  4.63164818  4.29094029\n",
            "   3.4738469  -4.31800609  4.68391297 -3.67316199 -3.48294623  1.92746715\n",
            "   2.78179132  1.07379666 -2.47412202  3.75153801  1.94690928  1.64438166\n",
            "  -1.05622589  0.84979956  0.65234749  2.14098663  1.6881807   3.44959609\n",
            "   2.47116382  3.41513531 -4.56660239  3.49501202  4.12248746  1.18194548\n",
            "   1.56875381  3.12870737 -0.09465791  2.94071492  3.7030081   1.54371028\n",
            "   1.45652041  3.72899383  1.16979546  2.96443568  3.50762486  4.30596366\n",
            "   1.73265795 -0.2373696   3.86771317  0.27795573 -6.60204283  2.22274003\n",
            "   3.70114881  3.0719749  -3.3373347   1.24348104  2.45757206  4.25939791\n",
            "   2.27841533  4.65009102  4.31996536  4.93131786 -0.88341345  2.30582916\n",
            "   3.32505968  0.43437624  2.37306232  2.33701177  3.25836872  2.52399466\n",
            "  -1.52221761 -1.73670812  0.30648313  2.3340165   3.88535771  2.56101572\n",
            "   3.81873056  1.79502943  3.9830403   2.05885304  3.86123095  0.0996433\n",
            "   3.58405557  3.34044943  4.60574901  5.19640043  4.49756673  0.0451981\n",
            "   3.1478115  -0.85689185  3.82579857 -1.01961361 -0.60759911 -1.36907289\n",
            "  -5.10694932  2.06394732 -0.19665815  4.33187305  2.32489912  2.03414425\n",
            "  -1.51660601  0.0778435   2.17912698  2.44180358  2.16433849 -3.04563706\n",
            "   2.07754174  5.10115296  3.53567373  3.310621    3.14621541  3.4110415 ]\n",
            " [ 3.97034555  4.13282077  4.06707016  4.06707016  4.73155874  3.57345417\n",
            "   2.97094164 -6.23353954  4.66349273 -3.81545965 -3.80542116 -0.02067543\n",
            "   2.10773558 -1.06321623 -1.56548203  3.46852266  1.93259436  2.03527569\n",
            "  -1.85720091  0.56094892 -1.71324337  2.36090763  1.1730168   5.01259902\n",
            "   2.9101527   3.34695904 -4.82040379  3.25860411  3.39394204 -0.47723341\n",
            "   2.21792906  4.37570536 -1.2432081   4.60162269  2.52824941  2.65555532\n",
            "   1.36568875  3.24100929  0.48286891  3.80629973  0.26357077  2.60204402\n",
            "   1.38337028  0.58411234  2.32582362  0.2366228  -5.16535804  3.23211608\n",
            "   2.75761827  1.86304508 -5.90303888  2.14381046  1.6370648   3.82386053\n",
            "   2.85309965  2.02289612  4.39409646  5.25221278 -1.4240235  -4.16905397\n",
            "   3.81804257 -0.55858914  2.46374897  2.92741851  3.68474093  1.16584812\n",
            "  -5.77073819 -0.25100607  0.85601037  2.91951657  3.02739844  3.26521823\n",
            "   4.51328171  0.10055934  3.53983522  2.23291559  2.69185404  0.36491857\n",
            "   2.75015526  2.81670925  5.57038255  4.52333132  3.00985135  0.02592262\n",
            "   2.47590801 -3.07918527  1.61990612 -2.02149623 -0.3763153  -2.40425876\n",
            "  -4.13383804  2.33055436  1.15092155  2.86605652  1.94299307  1.97329928\n",
            "  -0.70299578  0.05804601  2.0794838   3.12370469  2.39192988 -4.27874828\n",
            "   1.16356229  3.80816458  3.12546865  4.26710199  4.49749879  3.9741751 ]]\n",
            "KKKKKKKKKKKKKKKKKKKKKKKKKK\n",
            "[[ 4.0206418e+00  4.3550305e+00  4.4134731e+00  4.2981577e+00\n",
            "   4.8400717e+00  4.8181357e+00  4.2200003e+00 -4.1956134e+00\n",
            "   4.8169188e+00  2.9492433e+00 -2.7699971e+00  1.6064142e+00\n",
            "   2.8702564e+00 -2.2956792e-02 -1.7749998e-01  4.1526947e+00\n",
            "   2.3855751e+00  1.6537279e+00  1.1461183e+00  1.3127850e+00\n",
            "  -2.8592677e+00  2.4775622e+00  1.9083440e+00  5.6743631e+00\n",
            "   3.6498392e+00  4.2392168e+00 -9.5839840e-01  4.4227700e+00\n",
            "   4.5197425e+00  1.7529016e+00  2.3543365e+00  4.4355378e+00\n",
            "   7.1764016e-01  5.1741014e+00  1.7831578e+00  3.3637142e+00\n",
            "   1.9789073e+00  5.2140789e+00  2.9956021e+00  4.9826641e+00\n",
            "   3.8154562e+00  4.3235960e+00  1.4673370e+00  2.4193752e+00\n",
            "   3.7015872e+00  4.2058972e-01 -4.5381866e+00  3.4236410e+00\n",
            "   4.9438286e+00  3.7020490e+00 -3.2438068e+00  4.0639911e+00\n",
            "   2.4993804e+00  5.1971636e+00  3.0413005e+00  3.4728904e+00\n",
            "   4.1312952e+00  5.5351281e+00 -2.4291614e-01  6.1845914e-03\n",
            "   3.5988398e+00  2.5014958e-01  2.8982394e+00  4.5975685e+00\n",
            "   2.0436115e+00  1.4581606e+00 -3.9781408e+00  7.8033233e-01\n",
            "   3.5046341e+00  3.8723257e+00  4.3003135e+00  4.1434155e+00\n",
            "   4.2569933e+00  6.9904596e-01  4.3078065e+00  2.8780971e+00\n",
            "   4.4877286e+00  6.2076223e-01  4.8160262e+00  1.3815213e+00\n",
            "   5.9929209e+00  5.7671051e+00  4.4222994e+00  3.2929404e+00\n",
            "   3.8832397e+00 -2.2497694e+00  4.6418905e+00 -1.7253108e+00\n",
            "   1.2287380e+00 -2.3145049e+00 -3.5624254e+00  2.8368516e+00\n",
            "   2.2675791e+00  4.1826935e+00  3.5734098e+00  1.1418757e+00\n",
            "   7.4486285e-01  6.8887994e-02  2.4340451e+00  4.5754886e+00\n",
            "   1.8432655e+00 -2.2864769e+00  2.6277001e+00  5.0781789e+00\n",
            "   5.0276418e+00  4.4456110e+00  4.1314569e+00  3.3999388e+00]\n",
            " [ 3.8260429e+00  4.2446275e+00  4.2236567e+00  4.0947857e+00\n",
            "   4.6091776e+00  4.6015759e+00  3.9954004e+00 -3.9940276e+00\n",
            "   4.5863185e+00  2.8642170e+00 -2.6420913e+00  1.5735911e+00\n",
            "   2.7298579e+00 -6.1088435e-02 -1.7106093e-01  4.0063963e+00\n",
            "   2.2912223e+00  1.6143500e+00  1.1361915e+00  1.2094555e+00\n",
            "  -2.7569559e+00  2.3746316e+00  1.8334634e+00  5.4385977e+00\n",
            "   3.5209074e+00  4.0396376e+00 -8.4440243e-01  4.2455831e+00\n",
            "   4.3380237e+00  1.6574292e+00  2.2610111e+00  4.2446737e+00\n",
            "   6.6296238e-01  4.9271665e+00  1.7417613e+00  3.2308228e+00\n",
            "   1.8968700e+00  4.9554796e+00  2.8411224e+00  4.7690716e+00\n",
            "   3.6562974e+00  4.1181302e+00  1.3863730e+00  2.2851593e+00\n",
            "   3.5558183e+00  4.1289425e-01 -4.2856255e+00  3.2353272e+00\n",
            "   4.7655673e+00  3.4950693e+00 -3.1004512e+00  3.8459337e+00\n",
            "   2.3552165e+00  4.9981575e+00  2.9240537e+00  3.2605507e+00\n",
            "   3.9711373e+00  5.2568498e+00 -2.4355477e-01  5.9598996e-03\n",
            "   3.3915315e+00  2.3867983e-01  2.7716815e+00  4.3362999e+00\n",
            "   1.9396555e+00  1.3764437e+00 -3.8012676e+00  7.6656437e-01\n",
            "   3.3565753e+00  3.6638336e+00  4.1013436e+00  3.9869659e+00\n",
            "   4.0887589e+00  6.2484610e-01  4.0854154e+00  2.7287803e+00\n",
            "   4.2708802e+00  6.4385557e-01  4.5570331e+00  1.3559968e+00\n",
            "   5.7180772e+00  5.5208230e+00  4.2481294e+00  3.1497064e+00\n",
            "   3.7546043e+00 -2.1339946e+00  4.4262486e+00 -1.6444488e+00\n",
            "   1.1810105e+00 -2.2390854e+00 -3.3973939e+00  2.7033257e+00\n",
            "   2.1978877e+00  3.9852083e+00  3.4121377e+00  1.0705538e+00\n",
            "   7.4682355e-01  8.7484375e-02  2.3237870e+00  4.3412814e+00\n",
            "   1.7810427e+00 -2.1691451e+00  2.4768844e+00  4.8708291e+00\n",
            "   4.7981801e+00  4.1964684e+00  3.9138205e+00  3.3314703e+00]\n",
            " [ 3.4567859e+00  3.7943473e+00  3.7773738e+00  3.7585082e+00\n",
            "   4.1554365e+00  4.1458826e+00  3.6330171e+00 -3.6534014e+00\n",
            "   4.1277552e+00  2.5786581e+00 -2.3720655e+00  1.3468511e+00\n",
            "   2.4921782e+00  5.3244131e-04 -1.2722205e-01  3.5663178e+00\n",
            "   2.0913544e+00  1.4875230e+00  1.0025926e+00  1.1505163e+00\n",
            "  -2.4718609e+00  2.1645155e+00  1.6387401e+00  4.9002585e+00\n",
            "   3.1643028e+00  3.6791036e+00 -8.4585059e-01  3.8021960e+00\n",
            "   3.8348749e+00  1.5181582e+00  2.0683928e+00  3.7698786e+00\n",
            "   6.2909400e-01  4.4786897e+00  1.5799454e+00  2.8813527e+00\n",
            "   1.7191099e+00  4.4724159e+00  2.6172833e+00  4.2967639e+00\n",
            "   3.3393462e+00  3.7750907e+00  1.3123046e+00  2.1094396e+00\n",
            "   3.1640420e+00  3.3818856e-01 -3.9848945e+00  2.9358149e+00\n",
            "   4.2967072e+00  3.2331464e+00 -2.8788610e+00  3.4860544e+00\n",
            "   2.1551673e+00  4.4779816e+00  2.5725212e+00  3.0288029e+00\n",
            "   3.5331612e+00  4.7925806e+00 -2.4463589e-01  1.4347522e-02\n",
            "   3.0602920e+00  2.6113784e-01  2.5007586e+00  3.9556775e+00\n",
            "   1.8002412e+00  1.2639959e+00 -3.3921432e+00  6.7647666e-01\n",
            "   3.0361629e+00  3.3464460e+00  3.7044644e+00  3.5395594e+00\n",
            "   3.7164309e+00  5.6782389e-01  3.7098086e+00  2.5445819e+00\n",
            "   3.8778572e+00  5.0875425e-01  4.0967121e+00  1.1602243e+00\n",
            "   5.1641369e+00  4.9985495e+00  3.8226776e+00  2.8361058e+00\n",
            "   3.3260355e+00 -1.9642227e+00  4.0523443e+00 -1.5237677e+00\n",
            "   1.0454055e+00 -1.9835145e+00 -3.1135218e+00  2.4704945e+00\n",
            "   1.9845607e+00  3.5887811e+00  3.0679450e+00  1.0051999e+00\n",
            "   6.6372913e-01  7.6556660e-02  2.1181748e+00  3.9492064e+00\n",
            "   1.6433166e+00 -1.9526399e+00  2.2561638e+00  4.3521142e+00\n",
            "   4.3559632e+00  3.8059285e+00  3.5704074e+00  2.9388301e+00]\n",
            " [ 3.4960568e+00  3.8717759e+00  3.8376820e+00  3.7745156e+00\n",
            "   4.1983013e+00  4.1923032e+00  3.6664038e+00 -3.6692905e+00\n",
            "   4.1601200e+00  2.6019771e+00 -2.4051206e+00  1.4122761e+00\n",
            "   2.4993362e+00 -4.5372851e-02 -1.5324627e-01  3.6316998e+00\n",
            "   2.0997303e+00  1.4789526e+00  1.0262741e+00  1.1378437e+00\n",
            "  -2.4954026e+00  2.1837051e+00  1.6679795e+00  4.9544868e+00\n",
            "   3.2224023e+00  3.6810791e+00 -8.1579530e-01  3.8590910e+00\n",
            "   3.9029272e+00  1.5259019e+00  2.0839465e+00  3.8332410e+00\n",
            "   6.3580120e-01  4.5141244e+00  1.5807498e+00  2.9209616e+00\n",
            "   1.7266712e+00  4.5188355e+00  2.6232798e+00  4.3533869e+00\n",
            "   3.3452051e+00  3.7939432e+00  1.2950467e+00  2.1207075e+00\n",
            "   3.2276776e+00  3.5381216e-01 -3.9682193e+00  2.9486625e+00\n",
            "   4.3426342e+00  3.2138925e+00 -2.8551650e+00  3.5155897e+00\n",
            "   2.1684659e+00  4.5441904e+00  2.6382210e+00  3.0023227e+00\n",
            "   3.6240809e+00  4.8246183e+00 -2.3457889e-01  4.0577478e-03\n",
            "   3.1353354e+00  2.2347020e-01  2.5250332e+00  3.9938536e+00\n",
            "   1.8122755e+00  1.2705709e+00 -3.4728742e+00  6.9642031e-01\n",
            "   3.0785103e+00  3.3842940e+00  3.7348316e+00  3.6077344e+00\n",
            "   3.7241118e+00  5.7032436e-01  3.7488666e+00  2.5084176e+00\n",
            "   3.9252961e+00  5.6147170e-01  4.1752887e+00  1.2201326e+00\n",
            "   5.2395887e+00  5.0415058e+00  3.8800719e+00  2.8599262e+00\n",
            "   3.4087877e+00 -1.9432974e+00  4.0736828e+00 -1.5105644e+00\n",
            "   1.0754120e+00 -2.0291145e+00 -3.1287887e+00  2.4556379e+00\n",
            "   1.9825318e+00  3.6281290e+00  3.1043041e+00  9.8825526e-01\n",
            "   6.6101915e-01  8.3893456e-02  2.1416991e+00  3.9908059e+00\n",
            "   1.6382580e+00 -1.9764317e+00  2.2692339e+00  4.4257417e+00\n",
            "   4.3644738e+00  3.8372254e+00  3.5823019e+00  3.0101442e+00]]\n",
            "KKKKKKKKKKKKKKKKKKKKKKKKKK\n",
            "[[ 3.14200123 -4.24168481  3.50820838  3.45301098 -0.82878683  3.55198812\n",
            "  -2.39865244 -6.12672762 -0.10672683  0.71976341 -3.14990379  0.88687742\n",
            "   1.96307042 -2.96602867 -0.96485306  3.0573246   1.80836617  0.69483106\n",
            "  -3.59916629  0.88428494 -4.14375598  1.32681998  1.15791418  3.10145464\n",
            "  -3.94394818 -0.45704923 -3.17973647  2.01345678  3.49290798  1.08630887\n",
            "   1.91519355  1.99006189 -0.89089204  3.10862606  0.31465888  1.96238638\n",
            "   1.18530034  3.04548214  0.68146498  3.21140606  3.70074931 -0.6222812\n",
            "   0.33585268 -0.49023033  2.30214294 -1.62807221 -4.09654536  2.57267535\n",
            "   4.62704694  3.50130149 -4.55075547  2.32779749 -3.0686732   2.30381479\n",
            "   2.52119496  3.81951609  3.41332524  3.67610632 -6.74182502 -5.01156443\n",
            "   1.5462474  -1.05207687  1.35996932  2.44520436  0.37169979 -0.35820057\n",
            "  -5.11575173 -0.6314072   0.32288053  1.46943856  2.76690953  3.37200363\n",
            "   1.7394532  -6.58334416  3.5319249   1.6127086   3.13503207 -0.66395155\n",
            "   4.06999663  1.15799549  4.73989636  4.31418599  1.44526233  0.28127454\n",
            "   2.64671056 -4.54198503  3.74632638 -3.03609444  1.52410275 -3.03882508\n",
            "  -2.73518963  1.30597321  2.11105062  2.67127132  3.57314248 -1.12735046\n",
            "  -1.6723929  -0.46496717  0.65868609  4.05736274  0.85526518 -2.72253459\n",
            "   3.54003411  5.17118197  5.18668262  3.66831296  3.82868818  1.1772571 ]\n",
            " [ 3.40599731  5.7134509   5.25723885  5.24045097  4.74172481  4.90486413\n",
            "   5.09260533 -5.02544379  5.72105329  2.18549181 -3.21533712  1.64194021\n",
            "   2.90787944  0.80145692 -0.59077416  5.30981217  2.28761019  0.42374435\n",
            "   2.7811058   0.78562168 -3.47333691  2.90658546  2.36947666  6.32784573\n",
            "   2.86011799  4.23956907  0.03411955  3.819755    4.24397878  2.2907991\n",
            "   0.21054544  3.32214512 -1.90970399  4.45317078  0.6683813   1.59365058\n",
            "   1.68440314  5.11113288  2.99692001  5.99923782  6.64309868  5.40295846\n",
            "   0.32490177  0.69211005  3.29979136 -1.60424629 -6.62905308  4.90352694\n",
            "   4.99688408  4.12495816 -2.9216337   3.43068149  2.60198514  5.95873753\n",
            "   2.83167517  4.34509765  3.82192717  6.32176238  0.12217197  2.80634905\n",
            "  -0.86734911 -0.21511189  3.53131945  2.87442117 -1.92782223  0.3331608\n",
            "  -4.27693843  0.93958593  1.16395248  3.26064057  3.99138537  5.40150642\n",
            "   4.2357398   2.43908912  5.01012806  3.44314736  5.89294117  1.61704812\n",
            "   4.24549075 -1.43617676  6.00028627  6.64315603  4.89968855  5.1618463\n",
            "   2.60143546 -2.46206637  2.71564655 -3.14757091  0.1206718  -2.06879665\n",
            "  -5.83264703  1.05089459  1.72550644  2.60879083  2.79398982  0.15725611\n",
            "  -1.78106163  1.14279554  1.38952026  3.71189082  0.87435186 -3.04563706\n",
            "   0.97094974  5.29104355  4.50302929  3.90080892  3.59288026  2.2498049 ]\n",
            " [ 1.96404004  4.18793665  3.72307462  3.83831099  4.63164818  4.29094029\n",
            "   3.4738469  -4.31800609  4.68391297 -3.67316199 -3.48294623  1.92746715\n",
            "   2.78179132  1.07379666 -2.47412202  3.75153801  1.94690928  1.64438166\n",
            "  -1.05622589  0.84979956  0.65234749  2.14098663  1.6881807   3.44959609\n",
            "   2.47116382  3.41513531 -4.56660239  3.49501202  4.12248746  1.18194548\n",
            "   1.56875381  3.12870737 -0.09465791  2.94071492  3.7030081   1.54371028\n",
            "   1.45652041  3.72899383  1.16979546  2.96443568  3.50762486  4.30596366\n",
            "   1.73265795 -0.2373696   3.86771317  0.27795573 -6.60204283  2.22274003\n",
            "   3.70114881  3.0719749  -3.3373347   1.24348104  2.45757206  4.25939791\n",
            "   2.27841533  4.65009102  4.31996536  4.93131786 -0.88341345  2.30582916\n",
            "   3.32505968  0.43437624  2.37306232  2.33701177  3.25836872  2.52399466\n",
            "  -1.52221761 -1.73670812  0.30648313  2.3340165   3.88535771  2.56101572\n",
            "   3.81873056  1.79502943  3.9830403   2.05885304  3.86123095  0.0996433\n",
            "   3.58405557  3.34044943  4.60574901  5.19640043  4.49756673  0.0451981\n",
            "   3.1478115  -0.85689185  3.82579857 -1.01961361 -0.60759911 -1.36907289\n",
            "  -5.10694932  2.06394732 -0.19665815  4.33187305  2.32489912  2.03414425\n",
            "  -1.51660601  0.0778435   2.17912698  2.44180358  2.16433849 -3.04563706\n",
            "   2.07754174  5.10115296  3.53567373  3.310621    3.14621541  3.4110415 ]\n",
            " [ 3.97034555  4.13282077  4.06707016  4.06707016  4.73155874  3.57345417\n",
            "   2.97094164 -6.23353954  4.66349273 -3.81545965 -3.80542116 -0.02067543\n",
            "   2.10773558 -1.06321623 -1.56548203  3.46852266  1.93259436  2.03527569\n",
            "  -1.85720091  0.56094892 -1.71324337  2.36090763  1.1730168   5.01259902\n",
            "   2.9101527   3.34695904 -4.82040379  3.25860411  3.39394204 -0.47723341\n",
            "   2.21792906  4.37570536 -1.2432081   4.60162269  2.52824941  2.65555532\n",
            "   1.36568875  3.24100929  0.48286891  3.80629973  0.26357077  2.60204402\n",
            "   1.38337028  0.58411234  2.32582362  0.2366228  -5.16535804  3.23211608\n",
            "   2.75761827  1.86304508 -5.90303888  2.14381046  1.6370648   3.82386053\n",
            "   2.85309965  2.02289612  4.39409646  5.25221278 -1.4240235  -4.16905397\n",
            "   3.81804257 -0.55858914  2.46374897  2.92741851  3.68474093  1.16584812\n",
            "  -5.77073819 -0.25100607  0.85601037  2.91951657  3.02739844  3.26521823\n",
            "   4.51328171  0.10055934  3.53983522  2.23291559  2.69185404  0.36491857\n",
            "   2.75015526  2.81670925  5.57038255  4.52333132  3.00985135  0.02592262\n",
            "   2.47590801 -3.07918527  1.61990612 -2.02149623 -0.3763153  -2.40425876\n",
            "  -4.13383804  2.33055436  1.15092155  2.86605652  1.94299307  1.97329928\n",
            "  -0.70299578  0.05804601  2.0794838   3.12370469  2.39192988 -4.27874828\n",
            "   1.16356229  3.80816458  3.12546865  4.26710199  4.49749879  3.9741751 ]]\n",
            "KKKKKKKKKKKKKKKKKKKKKKKKKK\n",
            "i will be here /tmp/y44lo5dw.h5\n",
            "AM BACK\n",
            "26\n",
            "108\n",
            "??????????????????????\n",
            "Epoch 1/300\n",
            "1/1 - 2s - loss: 13.4484 - mse: 13.4484 - val_loss: 11.9567 - val_mse: 11.9567 - 2s/epoch - 2s/step\n",
            "\n",
            "Epoch 2/300\n",
            "1/1 - 0s - loss: 13.3931 - mse: 13.3931 - val_loss: 11.9305 - val_mse: 11.9305 - 51ms/epoch - 51ms/step\n",
            "\n",
            "Epoch 3/300\n",
            "1/1 - 0s - loss: 13.3287 - mse: 13.3287 - val_loss: 11.9038 - val_mse: 11.9038 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 4/300\n",
            "1/1 - 0s - loss: 13.2542 - mse: 13.2542 - val_loss: 11.8612 - val_mse: 11.8612 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 5/300\n",
            "1/1 - 0s - loss: 13.1339 - mse: 13.1339 - val_loss: 11.8029 - val_mse: 11.8029 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 6/300\n",
            "1/1 - 0s - loss: 13.0880 - mse: 13.0880 - val_loss: 11.7308 - val_mse: 11.7308 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 7/300\n",
            "1/1 - 0s - loss: 12.8973 - mse: 12.8973 - val_loss: 11.6426 - val_mse: 11.6426 - 51ms/epoch - 51ms/step\n",
            "\n",
            "Epoch 8/300\n",
            "1/1 - 0s - loss: 12.7465 - mse: 12.7465 - val_loss: 11.5411 - val_mse: 11.5411 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 9/300\n",
            "1/1 - 0s - loss: 12.6105 - mse: 12.6105 - val_loss: 11.4225 - val_mse: 11.4225 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 10/300\n",
            "1/1 - 0s - loss: 12.2917 - mse: 12.2917 - val_loss: 11.2806 - val_mse: 11.2806 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 11/300\n",
            "1/1 - 0s - loss: 12.0859 - mse: 12.0859 - val_loss: 11.1052 - val_mse: 11.1052 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 12/300\n",
            "1/1 - 0s - loss: 11.8315 - mse: 11.8315 - val_loss: 10.9320 - val_mse: 10.9320 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 13/300\n",
            "1/1 - 0s - loss: 11.4971 - mse: 11.4971 - val_loss: 10.7788 - val_mse: 10.7788 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 14/300\n",
            "1/1 - 0s - loss: 11.2706 - mse: 11.2706 - val_loss: 10.6344 - val_mse: 10.6344 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 15/300\n",
            "1/1 - 0s - loss: 10.9897 - mse: 10.9897 - val_loss: 10.4369 - val_mse: 10.4369 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 16/300\n",
            "1/1 - 0s - loss: 10.7053 - mse: 10.7053 - val_loss: 10.2126 - val_mse: 10.2126 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 17/300\n",
            "1/1 - 0s - loss: 10.1167 - mse: 10.1167 - val_loss: 9.9986 - val_mse: 9.9986 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 18/300\n",
            "1/1 - 0s - loss: 9.7625 - mse: 9.7625 - val_loss: 9.7934 - val_mse: 9.7934 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 19/300\n",
            "1/1 - 0s - loss: 9.8929 - mse: 9.8929 - val_loss: 9.5833 - val_mse: 9.5833 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 20/300\n",
            "1/1 - 0s - loss: 9.2783 - mse: 9.2783 - val_loss: 9.3771 - val_mse: 9.3771 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 21/300\n",
            "1/1 - 0s - loss: 9.0608 - mse: 9.0608 - val_loss: 9.1900 - val_mse: 9.1900 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 22/300\n",
            "1/1 - 0s - loss: 8.7485 - mse: 8.7485 - val_loss: 9.0334 - val_mse: 9.0334 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 23/300\n",
            "1/1 - 0s - loss: 8.2253 - mse: 8.2253 - val_loss: 8.8966 - val_mse: 8.8966 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 24/300\n",
            "1/1 - 0s - loss: 7.9699 - mse: 7.9699 - val_loss: 8.7512 - val_mse: 8.7512 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 25/300\n",
            "1/1 - 0s - loss: 7.9118 - mse: 7.9118 - val_loss: 8.5908 - val_mse: 8.5908 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 26/300\n",
            "1/1 - 0s - loss: 7.2651 - mse: 7.2651 - val_loss: 8.4095 - val_mse: 8.4095 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 27/300\n",
            "1/1 - 0s - loss: 7.2089 - mse: 7.2089 - val_loss: 8.2123 - val_mse: 8.2123 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 28/300\n",
            "1/1 - 0s - loss: 6.9242 - mse: 6.9242 - val_loss: 8.0335 - val_mse: 8.0335 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 29/300\n",
            "1/1 - 0s - loss: 6.8932 - mse: 6.8932 - val_loss: 7.8965 - val_mse: 7.8965 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 30/300\n",
            "1/1 - 0s - loss: 6.4937 - mse: 6.4937 - val_loss: 7.7754 - val_mse: 7.7754 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 31/300\n",
            "1/1 - 0s - loss: 6.4464 - mse: 6.4464 - val_loss: 7.6664 - val_mse: 7.6664 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 32/300\n",
            "1/1 - 0s - loss: 6.1904 - mse: 6.1904 - val_loss: 7.6053 - val_mse: 7.6053 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 33/300\n",
            "1/1 - 0s - loss: 5.9833 - mse: 5.9833 - val_loss: 7.5337 - val_mse: 7.5337 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 34/300\n",
            "1/1 - 0s - loss: 5.7610 - mse: 5.7610 - val_loss: 7.4538 - val_mse: 7.4538 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 35/300\n",
            "1/1 - 0s - loss: 5.6040 - mse: 5.6040 - val_loss: 7.3348 - val_mse: 7.3348 - 63ms/epoch - 63ms/step\n",
            "\n",
            "Epoch 36/300\n",
            "1/1 - 0s - loss: 5.4240 - mse: 5.4240 - val_loss: 7.1641 - val_mse: 7.1641 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 37/300\n",
            "1/1 - 0s - loss: 5.1636 - mse: 5.1636 - val_loss: 6.9959 - val_mse: 6.9959 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 38/300\n",
            "1/1 - 0s - loss: 5.1670 - mse: 5.1670 - val_loss: 6.9044 - val_mse: 6.9044 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 39/300\n",
            "1/1 - 0s - loss: 4.8375 - mse: 4.8375 - val_loss: 6.8181 - val_mse: 6.8181 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 40/300\n",
            "1/1 - 0s - loss: 4.5483 - mse: 4.5483 - val_loss: 6.7754 - val_mse: 6.7754 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 41/300\n",
            "1/1 - 0s - loss: 4.7070 - mse: 4.7070 - val_loss: 6.6978 - val_mse: 6.6978 - 60ms/epoch - 60ms/step\n",
            "\n",
            "Epoch 42/300\n",
            "1/1 - 0s - loss: 4.5207 - mse: 4.5207 - val_loss: 6.5930 - val_mse: 6.5930 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 43/300\n",
            "1/1 - 0s - loss: 4.3980 - mse: 4.3980 - val_loss: 6.4997 - val_mse: 6.4997 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 44/300\n",
            "1/1 - 0s - loss: 4.2556 - mse: 4.2556 - val_loss: 6.4533 - val_mse: 6.4533 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 45/300\n",
            "1/1 - 0s - loss: 4.1800 - mse: 4.1800 - val_loss: 6.4479 - val_mse: 6.4479 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 46/300\n",
            "1/1 - 0s - loss: 4.1597 - mse: 4.1597 - val_loss: 6.4484 - val_mse: 6.4484 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 47/300\n",
            "1/1 - 0s - loss: 3.9809 - mse: 3.9809 - val_loss: 6.4046 - val_mse: 6.4046 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 48/300\n",
            "1/1 - 0s - loss: 3.9672 - mse: 3.9672 - val_loss: 6.2798 - val_mse: 6.2798 - 60ms/epoch - 60ms/step\n",
            "\n",
            "Epoch 49/300\n",
            "1/1 - 0s - loss: 4.1256 - mse: 4.1256 - val_loss: 6.1610 - val_mse: 6.1610 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 50/300\n",
            "1/1 - 0s - loss: 3.7437 - mse: 3.7437 - val_loss: 6.0648 - val_mse: 6.0648 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 51/300\n",
            "1/1 - 0s - loss: 4.0216 - mse: 4.0216 - val_loss: 6.0039 - val_mse: 6.0039 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 52/300\n",
            "1/1 - 0s - loss: 3.7919 - mse: 3.7919 - val_loss: 5.9566 - val_mse: 5.9566 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 53/300\n",
            "1/1 - 0s - loss: 3.5981 - mse: 3.5981 - val_loss: 5.9263 - val_mse: 5.9263 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 54/300\n",
            "1/1 - 0s - loss: 3.4229 - mse: 3.4229 - val_loss: 5.9532 - val_mse: 5.9532 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 55/300\n",
            "1/1 - 0s - loss: 3.7317 - mse: 3.7317 - val_loss: 5.9253 - val_mse: 5.9253 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 56/300\n",
            "1/1 - 0s - loss: 3.3969 - mse: 3.3969 - val_loss: 5.8267 - val_mse: 5.8267 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 57/300\n",
            "1/1 - 0s - loss: 3.2210 - mse: 3.2210 - val_loss: 5.7311 - val_mse: 5.7311 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 58/300\n",
            "1/1 - 0s - loss: 3.0579 - mse: 3.0579 - val_loss: 5.6496 - val_mse: 5.6496 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 59/300\n",
            "1/1 - 0s - loss: 3.2965 - mse: 3.2965 - val_loss: 5.6061 - val_mse: 5.6061 - 59ms/epoch - 59ms/step\n",
            "\n",
            "Epoch 60/300\n",
            "1/1 - 0s - loss: 3.1535 - mse: 3.1535 - val_loss: 5.5804 - val_mse: 5.5804 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 61/300\n",
            "1/1 - 0s - loss: 2.9648 - mse: 2.9648 - val_loss: 5.5558 - val_mse: 5.5558 - 49ms/epoch - 49ms/step\n",
            "\n",
            "Epoch 62/300\n",
            "1/1 - 0s - loss: 3.3704 - mse: 3.3704 - val_loss: 5.5170 - val_mse: 5.5170 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 63/300\n",
            "1/1 - 0s - loss: 2.9158 - mse: 2.9158 - val_loss: 5.4915 - val_mse: 5.4915 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 64/300\n",
            "1/1 - 0s - loss: 2.8423 - mse: 2.8423 - val_loss: 5.4552 - val_mse: 5.4552 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 65/300\n",
            "1/1 - 0s - loss: 2.7493 - mse: 2.7493 - val_loss: 5.4344 - val_mse: 5.4344 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 66/300\n",
            "1/1 - 0s - loss: 2.9416 - mse: 2.9416 - val_loss: 5.4102 - val_mse: 5.4102 - 56ms/epoch - 56ms/step\n",
            "\n",
            "Epoch 67/300\n",
            "1/1 - 0s - loss: 2.7751 - mse: 2.7751 - val_loss: 5.3920 - val_mse: 5.3920 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 68/300\n",
            "1/1 - 0s - loss: 2.7961 - mse: 2.7961 - val_loss: 5.3675 - val_mse: 5.3675 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 69/300\n",
            "1/1 - 0s - loss: 2.6601 - mse: 2.6601 - val_loss: 5.3465 - val_mse: 5.3465 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 70/300\n",
            "1/1 - 0s - loss: 2.7643 - mse: 2.7643 - val_loss: 5.3264 - val_mse: 5.3264 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 71/300\n",
            "1/1 - 0s - loss: 3.0366 - mse: 3.0366 - val_loss: 5.3141 - val_mse: 5.3141 - 58ms/epoch - 58ms/step\n",
            "\n",
            "Epoch 72/300\n",
            "1/1 - 0s - loss: 2.7188 - mse: 2.7188 - val_loss: 5.2981 - val_mse: 5.2981 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 73/300\n",
            "1/1 - 0s - loss: 2.6155 - mse: 2.6155 - val_loss: 5.2841 - val_mse: 5.2841 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 74/300\n",
            "1/1 - 0s - loss: 2.5744 - mse: 2.5744 - val_loss: 5.2723 - val_mse: 5.2723 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 75/300\n",
            "1/1 - 0s - loss: 2.9276 - mse: 2.9276 - val_loss: 5.2590 - val_mse: 5.2590 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 76/300\n",
            "1/1 - 0s - loss: 2.6413 - mse: 2.6413 - val_loss: 5.2461 - val_mse: 5.2461 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 77/300\n",
            "1/1 - 0s - loss: 2.6250 - mse: 2.6250 - val_loss: 5.2348 - val_mse: 5.2348 - 55ms/epoch - 55ms/step\n",
            "\n",
            "Epoch 78/300\n",
            "1/1 - 0s - loss: 2.6147 - mse: 2.6147 - val_loss: 5.2281 - val_mse: 5.2281 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 79/300\n",
            "1/1 - 0s - loss: 2.8383 - mse: 2.8383 - val_loss: 5.2404 - val_mse: 5.2404 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 80/300\n",
            "1/1 - 0s - loss: 2.7038 - mse: 2.7038 - val_loss: 5.2213 - val_mse: 5.2213 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 81/300\n",
            "1/1 - 0s - loss: 2.5050 - mse: 2.5050 - val_loss: 5.1781 - val_mse: 5.1781 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 82/300\n",
            "1/1 - 0s - loss: 2.7298 - mse: 2.7298 - val_loss: 5.1603 - val_mse: 5.1603 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 83/300\n",
            "1/1 - 0s - loss: 2.6004 - mse: 2.6004 - val_loss: 5.1471 - val_mse: 5.1471 - 57ms/epoch - 57ms/step\n",
            "\n",
            "Epoch 84/300\n",
            "1/1 - 0s - loss: 2.6296 - mse: 2.6296 - val_loss: 5.1486 - val_mse: 5.1486 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 85/300\n",
            "1/1 - 0s - loss: 2.5151 - mse: 2.5151 - val_loss: 5.1437 - val_mse: 5.1437 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 86/300\n",
            "1/1 - 0s - loss: 2.3382 - mse: 2.3382 - val_loss: 5.1172 - val_mse: 5.1172 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 87/300\n",
            "1/1 - 0s - loss: 2.6026 - mse: 2.6026 - val_loss: 5.0954 - val_mse: 5.0954 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 88/300\n",
            "1/1 - 0s - loss: 2.4268 - mse: 2.4268 - val_loss: 5.0712 - val_mse: 5.0712 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 89/300\n",
            "1/1 - 0s - loss: 2.4167 - mse: 2.4167 - val_loss: 5.0605 - val_mse: 5.0605 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 90/300\n",
            "1/1 - 0s - loss: 2.5067 - mse: 2.5067 - val_loss: 5.0657 - val_mse: 5.0657 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 91/300\n",
            "1/1 - 0s - loss: 2.4325 - mse: 2.4325 - val_loss: 5.0955 - val_mse: 5.0955 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 92/300\n",
            "1/1 - 0s - loss: 2.5926 - mse: 2.5926 - val_loss: 5.1169 - val_mse: 5.1169 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 93/300\n",
            "1/1 - 0s - loss: 2.3405 - mse: 2.3405 - val_loss: 5.0711 - val_mse: 5.0711 - 49ms/epoch - 49ms/step\n",
            "\n",
            "Epoch 94/300\n",
            "1/1 - 0s - loss: 2.5136 - mse: 2.5136 - val_loss: 5.0272 - val_mse: 5.0272 - 64ms/epoch - 64ms/step\n",
            "\n",
            "Epoch 95/300\n",
            "1/1 - 0s - loss: 2.3317 - mse: 2.3317 - val_loss: 5.0153 - val_mse: 5.0153 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 96/300\n",
            "1/1 - 0s - loss: 2.2721 - mse: 2.2721 - val_loss: 5.0116 - val_mse: 5.0116 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 97/300\n",
            "1/1 - 0s - loss: 2.2927 - mse: 2.2927 - val_loss: 5.0011 - val_mse: 5.0011 - 49ms/epoch - 49ms/step\n",
            "\n",
            "Epoch 98/300\n",
            "1/1 - 0s - loss: 2.4067 - mse: 2.4067 - val_loss: 4.9921 - val_mse: 4.9921 - 47ms/epoch - 47ms/step\n",
            "\n",
            "Epoch 99/300\n",
            "1/1 - 0s - loss: 2.3603 - mse: 2.3603 - val_loss: 5.0025 - val_mse: 5.0025 - 53ms/epoch - 53ms/step\n",
            "\n",
            "Epoch 100/300\n",
            "1/1 - 0s - loss: 2.4892 - mse: 2.4892 - val_loss: 5.0212 - val_mse: 5.0212 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 101/300\n",
            "1/1 - 0s - loss: 2.3122 - mse: 2.3122 - val_loss: 5.0225 - val_mse: 5.0225 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 102/300\n",
            "1/1 - 0s - loss: 2.3189 - mse: 2.3189 - val_loss: 4.9834 - val_mse: 4.9834 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 103/300\n",
            "1/1 - 0s - loss: 2.2821 - mse: 2.2821 - val_loss: 4.9779 - val_mse: 4.9779 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 104/300\n",
            "1/1 - 0s - loss: 2.2034 - mse: 2.2034 - val_loss: 4.9923 - val_mse: 4.9923 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 105/300\n",
            "1/1 - 0s - loss: 2.3851 - mse: 2.3851 - val_loss: 4.9973 - val_mse: 4.9973 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 106/300\n",
            "1/1 - 0s - loss: 2.5539 - mse: 2.5539 - val_loss: 4.9749 - val_mse: 4.9749 - 56ms/epoch - 56ms/step\n",
            "\n",
            "Epoch 107/300\n",
            "1/1 - 0s - loss: 2.1256 - mse: 2.1256 - val_loss: 4.9574 - val_mse: 4.9574 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 108/300\n",
            "1/1 - 0s - loss: 2.2321 - mse: 2.2321 - val_loss: 4.9592 - val_mse: 4.9592 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 109/300\n",
            "1/1 - 0s - loss: 2.3163 - mse: 2.3163 - val_loss: 4.9691 - val_mse: 4.9691 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 110/300\n",
            "1/1 - 0s - loss: 2.2468 - mse: 2.2468 - val_loss: 4.9704 - val_mse: 4.9704 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 111/300\n",
            "1/1 - 0s - loss: 2.1617 - mse: 2.1617 - val_loss: 4.9502 - val_mse: 4.9502 - 52ms/epoch - 52ms/step\n",
            "\n",
            "Epoch 112/300\n",
            "1/1 - 0s - loss: 2.1562 - mse: 2.1562 - val_loss: 4.9460 - val_mse: 4.9460 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 113/300\n",
            "1/1 - 0s - loss: 2.2669 - mse: 2.2669 - val_loss: 4.9433 - val_mse: 4.9433 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 114/300\n",
            "1/1 - 0s - loss: 2.3199 - mse: 2.3199 - val_loss: 4.9398 - val_mse: 4.9398 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 115/300\n",
            "1/1 - 0s - loss: 2.0983 - mse: 2.0983 - val_loss: 4.9391 - val_mse: 4.9391 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 116/300\n",
            "1/1 - 0s - loss: 2.1626 - mse: 2.1626 - val_loss: 4.9387 - val_mse: 4.9387 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 117/300\n",
            "1/1 - 0s - loss: 2.2009 - mse: 2.2009 - val_loss: 4.9517 - val_mse: 4.9517 - 63ms/epoch - 63ms/step\n",
            "\n",
            "Epoch 118/300\n",
            "1/1 - 0s - loss: 2.1557 - mse: 2.1557 - val_loss: 4.9784 - val_mse: 4.9784 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 119/300\n",
            "1/1 - 0s - loss: 2.1857 - mse: 2.1857 - val_loss: 5.0161 - val_mse: 5.0161 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 120/300\n",
            "1/1 - 0s - loss: 2.1661 - mse: 2.1661 - val_loss: 5.0341 - val_mse: 5.0341 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 121/300\n",
            "1/1 - 0s - loss: 2.0861 - mse: 2.0861 - val_loss: 5.0056 - val_mse: 5.0056 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 122/300\n",
            "1/1 - 0s - loss: 1.9586 - mse: 1.9586 - val_loss: 4.9655 - val_mse: 4.9655 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 123/300\n",
            "1/1 - 0s - loss: 2.2292 - mse: 2.2292 - val_loss: 4.9446 - val_mse: 4.9446 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 124/300\n",
            "1/1 - 0s - loss: 2.1010 - mse: 2.1010 - val_loss: 4.9449 - val_mse: 4.9449 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 125/300\n",
            "1/1 - 0s - loss: 2.2737 - mse: 2.2737 - val_loss: 4.9542 - val_mse: 4.9542 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 126/300\n",
            "1/1 - 0s - loss: 1.9427 - mse: 1.9427 - val_loss: 4.9636 - val_mse: 4.9636 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 127/300\n",
            "1/1 - 0s - loss: 2.0165 - mse: 2.0165 - val_loss: 4.9731 - val_mse: 4.9731 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 128/300\n",
            "1/1 - 0s - loss: 2.1385 - mse: 2.1385 - val_loss: 4.9921 - val_mse: 4.9921 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 129/300\n",
            "1/1 - 0s - loss: 2.0517 - mse: 2.0517 - val_loss: 4.9724 - val_mse: 4.9724 - 54ms/epoch - 54ms/step\n",
            "\n",
            "Epoch 130/300\n",
            "1/1 - 0s - loss: 2.0196 - mse: 2.0196 - val_loss: 4.9490 - val_mse: 4.9490 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 131/300\n",
            "1/1 - 0s - loss: 1.8398 - mse: 1.8398 - val_loss: 4.9154 - val_mse: 4.9154 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 132/300\n",
            "1/1 - 0s - loss: 2.0007 - mse: 2.0007 - val_loss: 4.8980 - val_mse: 4.8980 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 133/300\n",
            "1/1 - 0s - loss: 1.9719 - mse: 1.9719 - val_loss: 4.8880 - val_mse: 4.8880 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 134/300\n",
            "1/1 - 0s - loss: 2.0186 - mse: 2.0186 - val_loss: 4.8832 - val_mse: 4.8832 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 135/300\n",
            "1/1 - 0s - loss: 1.9581 - mse: 1.9581 - val_loss: 4.9053 - val_mse: 4.9053 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 136/300\n",
            "1/1 - 0s - loss: 1.8532 - mse: 1.8532 - val_loss: 4.9183 - val_mse: 4.9183 - 49ms/epoch - 49ms/step\n",
            "\n",
            "Epoch 137/300\n",
            "1/1 - 0s - loss: 2.0332 - mse: 2.0332 - val_loss: 4.8955 - val_mse: 4.8955 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 138/300\n",
            "1/1 - 0s - loss: 2.0727 - mse: 2.0727 - val_loss: 4.8580 - val_mse: 4.8580 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 139/300\n",
            "1/1 - 0s - loss: 2.1558 - mse: 2.1558 - val_loss: 4.8448 - val_mse: 4.8448 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 140/300\n",
            "1/1 - 0s - loss: 1.8973 - mse: 1.8973 - val_loss: 4.8459 - val_mse: 4.8459 - 55ms/epoch - 55ms/step\n",
            "\n",
            "Epoch 141/300\n",
            "1/1 - 0s - loss: 2.0496 - mse: 2.0496 - val_loss: 4.8546 - val_mse: 4.8546 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 142/300\n",
            "1/1 - 0s - loss: 1.9618 - mse: 1.9618 - val_loss: 4.8826 - val_mse: 4.8826 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 143/300\n",
            "1/1 - 0s - loss: 1.9737 - mse: 1.9737 - val_loss: 4.9111 - val_mse: 4.9111 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 144/300\n",
            "1/1 - 0s - loss: 2.0045 - mse: 2.0045 - val_loss: 4.8887 - val_mse: 4.8887 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 145/300\n",
            "1/1 - 0s - loss: 2.0164 - mse: 2.0164 - val_loss: 4.8441 - val_mse: 4.8441 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 146/300\n",
            "1/1 - 0s - loss: 2.1491 - mse: 2.1491 - val_loss: 4.8331 - val_mse: 4.8331 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 147/300\n",
            "1/1 - 0s - loss: 1.8786 - mse: 1.8786 - val_loss: 4.8310 - val_mse: 4.8310 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 148/300\n",
            "1/1 - 0s - loss: 1.7536 - mse: 1.7536 - val_loss: 4.8322 - val_mse: 4.8322 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 149/300\n",
            "1/1 - 0s - loss: 1.9548 - mse: 1.9548 - val_loss: 4.8525 - val_mse: 4.8525 - 60ms/epoch - 60ms/step\n",
            "\n",
            "Epoch 150/300\n",
            "1/1 - 0s - loss: 1.9640 - mse: 1.9640 - val_loss: 4.8905 - val_mse: 4.8905 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 151/300\n",
            "1/1 - 0s - loss: 1.9089 - mse: 1.9089 - val_loss: 4.9582 - val_mse: 4.9582 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 152/300\n",
            "1/1 - 0s - loss: 1.8680 - mse: 1.8680 - val_loss: 4.9250 - val_mse: 4.9250 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 153/300\n",
            "1/1 - 0s - loss: 1.8671 - mse: 1.8671 - val_loss: 4.8813 - val_mse: 4.8813 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 154/300\n",
            "1/1 - 0s - loss: 2.0151 - mse: 2.0151 - val_loss: 4.8464 - val_mse: 4.8464 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 155/300\n",
            "1/1 - 0s - loss: 1.8465 - mse: 1.8465 - val_loss: 4.8369 - val_mse: 4.8369 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 156/300\n",
            "1/1 - 0s - loss: 1.9474 - mse: 1.9474 - val_loss: 4.8436 - val_mse: 4.8436 - 58ms/epoch - 58ms/step\n",
            "\n",
            "Epoch 157/300\n",
            "1/1 - 0s - loss: 1.7969 - mse: 1.7969 - val_loss: 4.8676 - val_mse: 4.8676 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 158/300\n",
            "1/1 - 0s - loss: 1.7956 - mse: 1.7956 - val_loss: 4.9055 - val_mse: 4.9055 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 159/300\n",
            "1/1 - 0s - loss: 1.8290 - mse: 1.8290 - val_loss: 4.9036 - val_mse: 4.9036 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 160/300\n",
            "1/1 - 0s - loss: 1.9567 - mse: 1.9567 - val_loss: 4.8865 - val_mse: 4.8865 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 161/300\n",
            "1/1 - 0s - loss: 1.8270 - mse: 1.8270 - val_loss: 4.8298 - val_mse: 4.8298 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 162/300\n",
            "1/1 - 0s - loss: 1.7718 - mse: 1.7718 - val_loss: 4.8075 - val_mse: 4.8075 - 61ms/epoch - 61ms/step\n",
            "\n",
            "Epoch 163/300\n",
            "1/1 - 0s - loss: 1.8875 - mse: 1.8875 - val_loss: 4.7994 - val_mse: 4.7994 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 164/300\n",
            "1/1 - 0s - loss: 1.9926 - mse: 1.9926 - val_loss: 4.8219 - val_mse: 4.8219 - 49ms/epoch - 49ms/step\n",
            "\n",
            "Epoch 165/300\n",
            "1/1 - 0s - loss: 1.8008 - mse: 1.8008 - val_loss: 4.8709 - val_mse: 4.8709 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 166/300\n",
            "1/1 - 0s - loss: 1.7297 - mse: 1.7297 - val_loss: 4.8917 - val_mse: 4.8917 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 167/300\n",
            "1/1 - 0s - loss: 1.7821 - mse: 1.7821 - val_loss: 4.8346 - val_mse: 4.8346 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 168/300\n",
            "1/1 - 0s - loss: 1.8790 - mse: 1.8790 - val_loss: 4.8228 - val_mse: 4.8228 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 169/300\n",
            "1/1 - 0s - loss: 1.7898 - mse: 1.7898 - val_loss: 4.8329 - val_mse: 4.8329 - 74ms/epoch - 74ms/step\n",
            "\n",
            "Epoch 170/300\n",
            "1/1 - 0s - loss: 1.9486 - mse: 1.9486 - val_loss: 4.8511 - val_mse: 4.8511 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 171/300\n",
            "1/1 - 0s - loss: 1.6764 - mse: 1.6764 - val_loss: 4.8790 - val_mse: 4.8790 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 172/300\n",
            "1/1 - 0s - loss: 1.7227 - mse: 1.7227 - val_loss: 4.9335 - val_mse: 4.9335 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 173/300\n",
            "1/1 - 0s - loss: 1.7765 - mse: 1.7765 - val_loss: 4.9612 - val_mse: 4.9612 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 174/300\n",
            "1/1 - 0s - loss: 1.7487 - mse: 1.7487 - val_loss: 4.8945 - val_mse: 4.8945 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 175/300\n",
            "1/1 - 0s - loss: 1.8462 - mse: 1.8462 - val_loss: 4.8634 - val_mse: 4.8634 - 56ms/epoch - 56ms/step\n",
            "\n",
            "Epoch 176/300\n",
            "1/1 - 0s - loss: 1.6155 - mse: 1.6155 - val_loss: 4.8303 - val_mse: 4.8303 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 177/300\n",
            "1/1 - 0s - loss: 1.6526 - mse: 1.6526 - val_loss: 4.8014 - val_mse: 4.8014 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 178/300\n",
            "1/1 - 0s - loss: 1.7593 - mse: 1.7593 - val_loss: 4.8015 - val_mse: 4.8015 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 179/300\n",
            "1/1 - 0s - loss: 1.6083 - mse: 1.6083 - val_loss: 4.8123 - val_mse: 4.8123 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 180/300\n",
            "1/1 - 0s - loss: 1.9248 - mse: 1.9248 - val_loss: 4.8541 - val_mse: 4.8541 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 181/300\n",
            "1/1 - 0s - loss: 1.7109 - mse: 1.7109 - val_loss: 4.8960 - val_mse: 4.8960 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 182/300\n",
            "1/1 - 0s - loss: 1.7189 - mse: 1.7189 - val_loss: 4.8987 - val_mse: 4.8987 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 183/300\n",
            "1/1 - 0s - loss: 1.6954 - mse: 1.6954 - val_loss: 4.8513 - val_mse: 4.8513 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 184/300\n",
            "1/1 - 0s - loss: 1.7118 - mse: 1.7118 - val_loss: 4.8433 - val_mse: 4.8433 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 185/300\n",
            "1/1 - 0s - loss: 1.5403 - mse: 1.5403 - val_loss: 4.8090 - val_mse: 4.8090 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 186/300\n",
            "1/1 - 0s - loss: 1.6742 - mse: 1.6742 - val_loss: 4.8094 - val_mse: 4.8094 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 187/300\n",
            "1/1 - 0s - loss: 1.7931 - mse: 1.7931 - val_loss: 4.8477 - val_mse: 4.8477 - 51ms/epoch - 51ms/step\n",
            "\n",
            "Epoch 188/300\n",
            "1/1 - 0s - loss: 1.5862 - mse: 1.5862 - val_loss: 4.9363 - val_mse: 4.9363 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 189/300\n",
            "1/1 - 0s - loss: 1.5391 - mse: 1.5391 - val_loss: 4.9397 - val_mse: 4.9397 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 190/300\n",
            "1/1 - 0s - loss: 1.6247 - mse: 1.6247 - val_loss: 4.8646 - val_mse: 4.8646 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 191/300\n",
            "1/1 - 0s - loss: 1.5441 - mse: 1.5441 - val_loss: 4.8324 - val_mse: 4.8324 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 192/300\n",
            "1/1 - 0s - loss: 1.7595 - mse: 1.7595 - val_loss: 4.8473 - val_mse: 4.8473 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 193/300\n",
            "1/1 - 0s - loss: 1.6512 - mse: 1.6512 - val_loss: 4.8925 - val_mse: 4.8925 - 52ms/epoch - 52ms/step\n",
            "\n",
            "Epoch 194/300\n",
            "1/1 - 0s - loss: 1.6602 - mse: 1.6602 - val_loss: 4.9266 - val_mse: 4.9266 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 195/300\n",
            "1/1 - 0s - loss: 1.7036 - mse: 1.7036 - val_loss: 4.8857 - val_mse: 4.8857 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 196/300\n",
            "1/1 - 0s - loss: 1.7587 - mse: 1.7587 - val_loss: 4.8365 - val_mse: 4.8365 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 197/300\n",
            "1/1 - 0s - loss: 1.6073 - mse: 1.6073 - val_loss: 4.7696 - val_mse: 4.7696 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 198/300\n",
            "1/1 - 0s - loss: 1.6509 - mse: 1.6509 - val_loss: 4.7374 - val_mse: 4.7374 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 199/300\n",
            "1/1 - 0s - loss: 1.6555 - mse: 1.6555 - val_loss: 4.7432 - val_mse: 4.7432 - 53ms/epoch - 53ms/step\n",
            "\n",
            "Epoch 200/300\n",
            "1/1 - 0s - loss: 1.4912 - mse: 1.4912 - val_loss: 4.7787 - val_mse: 4.7787 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 201/300\n",
            "1/1 - 0s - loss: 1.5764 - mse: 1.5764 - val_loss: 4.8540 - val_mse: 4.8540 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 202/300\n",
            "1/1 - 0s - loss: 1.5146 - mse: 1.5146 - val_loss: 4.8991 - val_mse: 4.8991 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 203/300\n",
            "1/1 - 0s - loss: 1.6393 - mse: 1.6393 - val_loss: 4.8958 - val_mse: 4.8958 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 204/300\n",
            "1/1 - 0s - loss: 1.5624 - mse: 1.5624 - val_loss: 4.8576 - val_mse: 4.8576 - 56ms/epoch - 56ms/step\n",
            "\n",
            "Epoch 205/300\n",
            "1/1 - 0s - loss: 1.6526 - mse: 1.6526 - val_loss: 4.8847 - val_mse: 4.8847 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 206/300\n",
            "1/1 - 0s - loss: 1.5061 - mse: 1.5061 - val_loss: 4.9262 - val_mse: 4.9262 - 49ms/epoch - 49ms/step\n",
            "\n",
            "Epoch 207/300\n",
            "1/1 - 0s - loss: 1.4676 - mse: 1.4676 - val_loss: 4.9322 - val_mse: 4.9322 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 208/300\n",
            "1/1 - 0s - loss: 1.5068 - mse: 1.5068 - val_loss: 4.9872 - val_mse: 4.9872 - 47ms/epoch - 47ms/step\n",
            "\n",
            "Epoch 209/300\n",
            "1/1 - 0s - loss: 1.4272 - mse: 1.4272 - val_loss: 4.9740 - val_mse: 4.9740 - 48ms/epoch - 48ms/step\n",
            "\n",
            "Epoch 210/300\n",
            "1/1 - 0s - loss: 1.6413 - mse: 1.6413 - val_loss: 5.0076 - val_mse: 5.0076 - 55ms/epoch - 55ms/step\n",
            "\n",
            "Epoch 211/300\n",
            "1/1 - 0s - loss: 1.5989 - mse: 1.5989 - val_loss: 5.0581 - val_mse: 5.0581 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 212/300\n",
            "1/1 - 0s - loss: 1.6093 - mse: 1.6093 - val_loss: 5.0325 - val_mse: 5.0325 - 52ms/epoch - 52ms/step\n",
            "\n",
            "Epoch 213/300\n",
            "1/1 - 0s - loss: 1.5304 - mse: 1.5304 - val_loss: 4.9968 - val_mse: 4.9968 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 214/300\n",
            "1/1 - 0s - loss: 1.4317 - mse: 1.4317 - val_loss: 4.9116 - val_mse: 4.9116 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 215/300\n",
            "1/1 - 0s - loss: 1.6038 - mse: 1.6038 - val_loss: 4.8729 - val_mse: 4.8729 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 216/300\n",
            "1/1 - 0s - loss: 1.6432 - mse: 1.6432 - val_loss: 4.8867 - val_mse: 4.8867 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 217/300\n",
            "1/1 - 0s - loss: 1.4426 - mse: 1.4426 - val_loss: 4.9225 - val_mse: 4.9225 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 218/300\n",
            "1/1 - 0s - loss: 1.3822 - mse: 1.3822 - val_loss: 4.9190 - val_mse: 4.9190 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 219/300\n",
            "1/1 - 0s - loss: 1.6105 - mse: 1.6105 - val_loss: 4.8705 - val_mse: 4.8705 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 220/300\n",
            "1/1 - 0s - loss: 1.4290 - mse: 1.4290 - val_loss: 4.7609 - val_mse: 4.7609 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 221/300\n",
            "1/1 - 0s - loss: 1.4400 - mse: 1.4400 - val_loss: 4.7017 - val_mse: 4.7017 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 222/300\n",
            "1/1 - 0s - loss: 1.5175 - mse: 1.5175 - val_loss: 4.7036 - val_mse: 4.7036 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 223/300\n",
            "1/1 - 0s - loss: 1.5582 - mse: 1.5582 - val_loss: 4.7774 - val_mse: 4.7774 - 54ms/epoch - 54ms/step\n",
            "\n",
            "Epoch 224/300\n",
            "1/1 - 0s - loss: 1.5304 - mse: 1.5304 - val_loss: 4.9317 - val_mse: 4.9317 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 225/300\n",
            "1/1 - 0s - loss: 1.5524 - mse: 1.5524 - val_loss: 5.0343 - val_mse: 5.0343 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 226/300\n",
            "1/1 - 0s - loss: 1.6786 - mse: 1.6786 - val_loss: 4.9158 - val_mse: 4.9158 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 227/300\n",
            "1/1 - 0s - loss: 1.6200 - mse: 1.6200 - val_loss: 4.7697 - val_mse: 4.7697 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 228/300\n",
            "1/1 - 0s - loss: 1.4674 - mse: 1.4674 - val_loss: 4.7121 - val_mse: 4.7121 - 49ms/epoch - 49ms/step\n",
            "\n",
            "Epoch 229/300\n",
            "1/1 - 0s - loss: 1.6501 - mse: 1.6501 - val_loss: 4.7356 - val_mse: 4.7356 - 47ms/epoch - 47ms/step\n",
            "\n",
            "Epoch 230/300\n",
            "1/1 - 0s - loss: 1.5284 - mse: 1.5284 - val_loss: 4.7806 - val_mse: 4.7806 - 50ms/epoch - 50ms/step\n",
            "\n",
            "Epoch 231/300\n",
            "1/1 - 0s - loss: 1.6140 - mse: 1.6140 - val_loss: 4.9467 - val_mse: 4.9467 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 232/300\n",
            "1/1 - 0s - loss: 1.6205 - mse: 1.6205 - val_loss: 5.1260 - val_mse: 5.1260 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 233/300\n",
            "1/1 - 0s - loss: 1.6100 - mse: 1.6100 - val_loss: 5.0895 - val_mse: 5.0895 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 234/300\n",
            "1/1 - 0s - loss: 1.5951 - mse: 1.5951 - val_loss: 4.9573 - val_mse: 4.9573 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 235/300\n",
            "1/1 - 0s - loss: 1.4348 - mse: 1.4348 - val_loss: 4.8560 - val_mse: 4.8560 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 236/300\n",
            "1/1 - 0s - loss: 1.5668 - mse: 1.5668 - val_loss: 4.8351 - val_mse: 4.8351 - 55ms/epoch - 55ms/step\n",
            "\n",
            "Epoch 237/300\n",
            "1/1 - 0s - loss: 1.6000 - mse: 1.6000 - val_loss: 4.8593 - val_mse: 4.8593 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 238/300\n",
            "1/1 - 0s - loss: 1.3498 - mse: 1.3498 - val_loss: 4.8688 - val_mse: 4.8688 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 239/300\n",
            "1/1 - 0s - loss: 1.5553 - mse: 1.5553 - val_loss: 4.9446 - val_mse: 4.9446 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 240/300\n",
            "1/1 - 0s - loss: 1.6121 - mse: 1.6121 - val_loss: 5.0461 - val_mse: 5.0461 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 241/300\n",
            "1/1 - 0s - loss: 1.5206 - mse: 1.5206 - val_loss: 5.0440 - val_mse: 5.0440 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 242/300\n",
            "1/1 - 0s - loss: 1.3550 - mse: 1.3550 - val_loss: 4.9702 - val_mse: 4.9702 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 243/300\n",
            "1/1 - 0s - loss: 1.3355 - mse: 1.3355 - val_loss: 4.8834 - val_mse: 4.8834 - 55ms/epoch - 55ms/step\n",
            "\n",
            "Epoch 244/300\n",
            "1/1 - 0s - loss: 1.4232 - mse: 1.4232 - val_loss: 4.8610 - val_mse: 4.8610 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 245/300\n",
            "1/1 - 0s - loss: 1.4169 - mse: 1.4169 - val_loss: 4.8635 - val_mse: 4.8635 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 246/300\n",
            "1/1 - 0s - loss: 1.3575 - mse: 1.3575 - val_loss: 4.8711 - val_mse: 4.8711 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 247/300\n",
            "1/1 - 0s - loss: 1.4139 - mse: 1.4139 - val_loss: 4.9371 - val_mse: 4.9371 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 248/300\n",
            "1/1 - 0s - loss: 1.4554 - mse: 1.4554 - val_loss: 4.9870 - val_mse: 4.9870 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 249/300\n",
            "1/1 - 0s - loss: 1.3933 - mse: 1.3933 - val_loss: 5.0174 - val_mse: 5.0174 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 250/300\n",
            "1/1 - 0s - loss: 1.3766 - mse: 1.3766 - val_loss: 4.8918 - val_mse: 4.8918 - 54ms/epoch - 54ms/step\n",
            "\n",
            "Epoch 251/300\n",
            "1/1 - 0s - loss: 1.3436 - mse: 1.3436 - val_loss: 4.7486 - val_mse: 4.7486 - 52ms/epoch - 52ms/step\n",
            "\n",
            "Epoch 252/300\n",
            "1/1 - 0s - loss: 1.3670 - mse: 1.3670 - val_loss: 4.6931 - val_mse: 4.6931 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 253/300\n",
            "1/1 - 0s - loss: 1.4475 - mse: 1.4475 - val_loss: 4.6955 - val_mse: 4.6955 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 254/300\n",
            "1/1 - 0s - loss: 1.4996 - mse: 1.4996 - val_loss: 4.7472 - val_mse: 4.7472 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 255/300\n",
            "1/1 - 0s - loss: 1.6362 - mse: 1.6362 - val_loss: 4.8694 - val_mse: 4.8694 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 256/300\n",
            "1/1 - 0s - loss: 1.3802 - mse: 1.3802 - val_loss: 4.9535 - val_mse: 4.9535 - 54ms/epoch - 54ms/step\n",
            "\n",
            "Epoch 257/300\n",
            "1/1 - 0s - loss: 1.3353 - mse: 1.3353 - val_loss: 4.9840 - val_mse: 4.9840 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 258/300\n",
            "1/1 - 0s - loss: 1.3866 - mse: 1.3866 - val_loss: 4.8780 - val_mse: 4.8780 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 259/300\n",
            "1/1 - 0s - loss: 1.3273 - mse: 1.3273 - val_loss: 4.7892 - val_mse: 4.7892 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 260/300\n",
            "1/1 - 0s - loss: 1.4460 - mse: 1.4460 - val_loss: 4.7549 - val_mse: 4.7549 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 261/300\n",
            "1/1 - 0s - loss: 1.3877 - mse: 1.3877 - val_loss: 4.7395 - val_mse: 4.7395 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 262/300\n",
            "1/1 - 0s - loss: 1.5174 - mse: 1.5174 - val_loss: 4.7647 - val_mse: 4.7647 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 263/300\n",
            "1/1 - 0s - loss: 1.4048 - mse: 1.4048 - val_loss: 4.8123 - val_mse: 4.8123 - 59ms/epoch - 59ms/step\n",
            "\n",
            "Epoch 264/300\n",
            "1/1 - 0s - loss: 1.2820 - mse: 1.2820 - val_loss: 4.8282 - val_mse: 4.8282 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 265/300\n",
            "1/1 - 0s - loss: 1.3807 - mse: 1.3807 - val_loss: 4.8412 - val_mse: 4.8412 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 266/300\n",
            "1/1 - 0s - loss: 1.4371 - mse: 1.4371 - val_loss: 4.8221 - val_mse: 4.8221 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 267/300\n",
            "1/1 - 0s - loss: 1.4236 - mse: 1.4236 - val_loss: 4.7958 - val_mse: 4.7958 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 268/300\n",
            "1/1 - 0s - loss: 1.2482 - mse: 1.2482 - val_loss: 4.7503 - val_mse: 4.7503 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 269/300\n",
            "1/1 - 0s - loss: 1.3471 - mse: 1.3471 - val_loss: 4.7336 - val_mse: 4.7336 - 51ms/epoch - 51ms/step\n",
            "\n",
            "Epoch 270/300\n",
            "1/1 - 0s - loss: 1.3095 - mse: 1.3095 - val_loss: 4.6841 - val_mse: 4.6841 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 271/300\n",
            "1/1 - 0s - loss: 1.4019 - mse: 1.4019 - val_loss: 4.6916 - val_mse: 4.6916 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 272/300\n",
            "1/1 - 0s - loss: 1.2341 - mse: 1.2341 - val_loss: 4.7189 - val_mse: 4.7189 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 273/300\n",
            "1/1 - 0s - loss: 1.2517 - mse: 1.2517 - val_loss: 4.7236 - val_mse: 4.7236 - 43ms/epoch - 43ms/step\n",
            "\n",
            "Epoch 274/300\n",
            "1/1 - 0s - loss: 1.3413 - mse: 1.3413 - val_loss: 4.7777 - val_mse: 4.7777 - 54ms/epoch - 54ms/step\n",
            "\n",
            "Epoch 275/300\n",
            "1/1 - 0s - loss: 1.3284 - mse: 1.3284 - val_loss: 4.8372 - val_mse: 4.8372 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 276/300\n",
            "1/1 - 0s - loss: 1.3591 - mse: 1.3591 - val_loss: 4.9021 - val_mse: 4.9021 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 277/300\n",
            "1/1 - 0s - loss: 1.2658 - mse: 1.2658 - val_loss: 4.8818 - val_mse: 4.8818 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 278/300\n",
            "1/1 - 0s - loss: 1.2647 - mse: 1.2647 - val_loss: 4.8764 - val_mse: 4.8764 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 279/300\n",
            "1/1 - 0s - loss: 1.3835 - mse: 1.3835 - val_loss: 4.8609 - val_mse: 4.8609 - 45ms/epoch - 45ms/step\n",
            "\n",
            "Epoch 280/300\n",
            "1/1 - 0s - loss: 1.1955 - mse: 1.1955 - val_loss: 4.8364 - val_mse: 4.8364 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 281/300\n",
            "1/1 - 0s - loss: 1.2362 - mse: 1.2362 - val_loss: 4.7815 - val_mse: 4.7815 - 66ms/epoch - 66ms/step\n",
            "\n",
            "Epoch 282/300\n",
            "1/1 - 0s - loss: 1.3449 - mse: 1.3449 - val_loss: 4.8018 - val_mse: 4.8018 - 44ms/epoch - 44ms/step\n",
            "\n",
            "Epoch 283/300\n",
            "1/1 - 0s - loss: 1.4202 - mse: 1.4202 - val_loss: 4.9379 - val_mse: 4.9379 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 284/300\n",
            "1/1 - 0s - loss: 1.2313 - mse: 1.2313 - val_loss: 5.1942 - val_mse: 5.1942 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 285/300\n",
            "1/1 - 0s - loss: 1.2367 - mse: 1.2367 - val_loss: 5.2985 - val_mse: 5.2985 - 53ms/epoch - 53ms/step\n",
            "\n",
            "Epoch 286/300\n",
            "1/1 - 0s - loss: 1.3643 - mse: 1.3643 - val_loss: 5.2363 - val_mse: 5.2363 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 287/300\n",
            "1/1 - 0s - loss: 1.3448 - mse: 1.3448 - val_loss: 5.0520 - val_mse: 5.0520 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 288/300\n",
            "1/1 - 0s - loss: 1.2586 - mse: 1.2586 - val_loss: 4.8765 - val_mse: 4.8765 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 289/300\n",
            "1/1 - 0s - loss: 1.3469 - mse: 1.3469 - val_loss: 4.8160 - val_mse: 4.8160 - 41ms/epoch - 41ms/step\n",
            "\n",
            "Epoch 290/300\n",
            "1/1 - 0s - loss: 1.2947 - mse: 1.2947 - val_loss: 4.8081 - val_mse: 4.8081 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 291/300\n",
            "1/1 - 0s - loss: 1.2111 - mse: 1.2111 - val_loss: 4.8422 - val_mse: 4.8422 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 292/300\n",
            "1/1 - 0s - loss: 1.3219 - mse: 1.3219 - val_loss: 4.9194 - val_mse: 4.9194 - 56ms/epoch - 56ms/step\n",
            "\n",
            "Epoch 293/300\n",
            "1/1 - 0s - loss: 1.2074 - mse: 1.2074 - val_loss: 5.0096 - val_mse: 5.0096 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 294/300\n",
            "1/1 - 0s - loss: 1.1842 - mse: 1.1842 - val_loss: 4.9774 - val_mse: 4.9774 - 42ms/epoch - 42ms/step\n",
            "\n",
            "Epoch 295/300\n",
            "1/1 - 0s - loss: 1.3329 - mse: 1.3329 - val_loss: 4.8931 - val_mse: 4.8931 - 40ms/epoch - 40ms/step\n",
            "\n",
            "Epoch 296/300\n",
            "1/1 - 0s - loss: 1.2563 - mse: 1.2563 - val_loss: 4.7870 - val_mse: 4.7870 - 56ms/epoch - 56ms/step\n",
            "\n",
            "Epoch 297/300\n",
            "1/1 - 0s - loss: 1.2971 - mse: 1.2971 - val_loss: 4.7014 - val_mse: 4.7014 - 38ms/epoch - 38ms/step\n",
            "\n",
            "Epoch 298/300\n",
            "1/1 - 0s - loss: 1.5209 - mse: 1.5209 - val_loss: 4.7009 - val_mse: 4.7009 - 46ms/epoch - 46ms/step\n",
            "\n",
            "Epoch 299/300\n",
            "1/1 - 0s - loss: 1.2330 - mse: 1.2330 - val_loss: 4.7063 - val_mse: 4.7063 - 39ms/epoch - 39ms/step\n",
            "\n",
            "Epoch 300/300\n",
            "1/1 - 0s - loss: 1.3520 - mse: 1.3520 - val_loss: 4.7645 - val_mse: 4.7645 - 41ms/epoch - 41ms/step\n",
            "\n",
            "RRRRRRRRRRRRRRRRRRR\n",
            "KKKKKKKKKKKKKKKKKKKKKKKKKK\n",
            "[[ 3.14200123 -4.24168481  3.50820838  3.45301098 -0.82878683  3.55198812\n",
            "  -2.39865244 -6.12672762 -0.10672683  0.71976341 -3.14990379  0.88687742\n",
            "   1.96307042 -2.96602867 -0.96485306  3.0573246   1.80836617  0.69483106\n",
            "  -3.59916629  0.88428494 -4.14375598  1.32681998  1.15791418  3.10145464\n",
            "  -3.94394818 -0.45704923 -3.17973647  2.01345678  3.49290798  1.08630887\n",
            "   1.91519355  1.99006189 -0.89089204  3.10862606  0.31465888  1.96238638\n",
            "   1.18530034  3.04548214  0.68146498  3.21140606  3.70074931 -0.6222812\n",
            "   0.33585268 -0.49023033  2.30214294 -1.62807221 -4.09654536  2.57267535\n",
            "   4.62704694  3.50130149 -4.55075547  2.32779749 -3.0686732   2.30381479\n",
            "   2.52119496  3.81951609  3.41332524  3.67610632 -6.74182502 -5.01156443\n",
            "   1.5462474  -1.05207687  1.35996932  2.44520436  0.37169979 -0.35820057\n",
            "  -5.11575173 -0.6314072   0.32288053  1.46943856  2.76690953  3.37200363\n",
            "   1.7394532  -6.58334416  3.5319249   1.6127086   3.13503207 -0.66395155\n",
            "   4.06999663  1.15799549  4.73989636  4.31418599  1.44526233  0.28127454\n",
            "   2.64671056 -4.54198503  3.74632638 -3.03609444  1.52410275 -3.03882508\n",
            "  -2.73518963  1.30597321  2.11105062  2.67127132  3.57314248 -1.12735046\n",
            "  -1.6723929  -0.46496717  0.65868609  4.05736274  0.85526518 -2.72253459\n",
            "   3.54003411  5.17118197  5.18668262  3.66831296  3.82868818  1.1772571 ]\n",
            " [ 3.40599731  5.7134509   5.25723885  5.24045097  4.74172481  4.90486413\n",
            "   5.09260533 -5.02544379  5.72105329  2.18549181 -3.21533712  1.64194021\n",
            "   2.90787944  0.80145692 -0.59077416  5.30981217  2.28761019  0.42374435\n",
            "   2.7811058   0.78562168 -3.47333691  2.90658546  2.36947666  6.32784573\n",
            "   2.86011799  4.23956907  0.03411955  3.819755    4.24397878  2.2907991\n",
            "   0.21054544  3.32214512 -1.90970399  4.45317078  0.6683813   1.59365058\n",
            "   1.68440314  5.11113288  2.99692001  5.99923782  6.64309868  5.40295846\n",
            "   0.32490177  0.69211005  3.29979136 -1.60424629 -6.62905308  4.90352694\n",
            "   4.99688408  4.12495816 -2.9216337   3.43068149  2.60198514  5.95873753\n",
            "   2.83167517  4.34509765  3.82192717  6.32176238  0.12217197  2.80634905\n",
            "  -0.86734911 -0.21511189  3.53131945  2.87442117 -1.92782223  0.3331608\n",
            "  -4.27693843  0.93958593  1.16395248  3.26064057  3.99138537  5.40150642\n",
            "   4.2357398   2.43908912  5.01012806  3.44314736  5.89294117  1.61704812\n",
            "   4.24549075 -1.43617676  6.00028627  6.64315603  4.89968855  5.1618463\n",
            "   2.60143546 -2.46206637  2.71564655 -3.14757091  0.1206718  -2.06879665\n",
            "  -5.83264703  1.05089459  1.72550644  2.60879083  2.79398982  0.15725611\n",
            "  -1.78106163  1.14279554  1.38952026  3.71189082  0.87435186 -3.04563706\n",
            "   0.97094974  5.29104355  4.50302929  3.90080892  3.59288026  2.2498049 ]\n",
            " [ 1.96404004  4.18793665  3.72307462  3.83831099  4.63164818  4.29094029\n",
            "   3.4738469  -4.31800609  4.68391297 -3.67316199 -3.48294623  1.92746715\n",
            "   2.78179132  1.07379666 -2.47412202  3.75153801  1.94690928  1.64438166\n",
            "  -1.05622589  0.84979956  0.65234749  2.14098663  1.6881807   3.44959609\n",
            "   2.47116382  3.41513531 -4.56660239  3.49501202  4.12248746  1.18194548\n",
            "   1.56875381  3.12870737 -0.09465791  2.94071492  3.7030081   1.54371028\n",
            "   1.45652041  3.72899383  1.16979546  2.96443568  3.50762486  4.30596366\n",
            "   1.73265795 -0.2373696   3.86771317  0.27795573 -6.60204283  2.22274003\n",
            "   3.70114881  3.0719749  -3.3373347   1.24348104  2.45757206  4.25939791\n",
            "   2.27841533  4.65009102  4.31996536  4.93131786 -0.88341345  2.30582916\n",
            "   3.32505968  0.43437624  2.37306232  2.33701177  3.25836872  2.52399466\n",
            "  -1.52221761 -1.73670812  0.30648313  2.3340165   3.88535771  2.56101572\n",
            "   3.81873056  1.79502943  3.9830403   2.05885304  3.86123095  0.0996433\n",
            "   3.58405557  3.34044943  4.60574901  5.19640043  4.49756673  0.0451981\n",
            "   3.1478115  -0.85689185  3.82579857 -1.01961361 -0.60759911 -1.36907289\n",
            "  -5.10694932  2.06394732 -0.19665815  4.33187305  2.32489912  2.03414425\n",
            "  -1.51660601  0.0778435   2.17912698  2.44180358  2.16433849 -3.04563706\n",
            "   2.07754174  5.10115296  3.53567373  3.310621    3.14621541  3.4110415 ]\n",
            " [ 3.97034555  4.13282077  4.06707016  4.06707016  4.73155874  3.57345417\n",
            "   2.97094164 -6.23353954  4.66349273 -3.81545965 -3.80542116 -0.02067543\n",
            "   2.10773558 -1.06321623 -1.56548203  3.46852266  1.93259436  2.03527569\n",
            "  -1.85720091  0.56094892 -1.71324337  2.36090763  1.1730168   5.01259902\n",
            "   2.9101527   3.34695904 -4.82040379  3.25860411  3.39394204 -0.47723341\n",
            "   2.21792906  4.37570536 -1.2432081   4.60162269  2.52824941  2.65555532\n",
            "   1.36568875  3.24100929  0.48286891  3.80629973  0.26357077  2.60204402\n",
            "   1.38337028  0.58411234  2.32582362  0.2366228  -5.16535804  3.23211608\n",
            "   2.75761827  1.86304508 -5.90303888  2.14381046  1.6370648   3.82386053\n",
            "   2.85309965  2.02289612  4.39409646  5.25221278 -1.4240235  -4.16905397\n",
            "   3.81804257 -0.55858914  2.46374897  2.92741851  3.68474093  1.16584812\n",
            "  -5.77073819 -0.25100607  0.85601037  2.91951657  3.02739844  3.26521823\n",
            "   4.51328171  0.10055934  3.53983522  2.23291559  2.69185404  0.36491857\n",
            "   2.75015526  2.81670925  5.57038255  4.52333132  3.00985135  0.02592262\n",
            "   2.47590801 -3.07918527  1.61990612 -2.02149623 -0.3763153  -2.40425876\n",
            "  -4.13383804  2.33055436  1.15092155  2.86605652  1.94299307  1.97329928\n",
            "  -0.70299578  0.05804601  2.0794838   3.12370469  2.39192988 -4.27874828\n",
            "   1.16356229  3.80816458  3.12546865  4.26710199  4.49749879  3.9741751 ]]\n",
            "KKKKKKKKKKKKKKKKKKKKKKKKKK\n",
            "[[ 3.4082513   3.8461666   3.7648618   3.5274239   4.1461887   4.046608\n",
            "   3.44684    -4.30539     4.150814    2.4979646  -2.550466    1.4563252\n",
            "   2.0670822  -0.29905552 -0.404288    3.8145251   2.2299693   1.4681312\n",
            "   0.02109016  0.98872244 -2.6775532   2.157994    1.7128706   4.9375334\n",
            "   3.140811    3.6568334  -1.3804395   3.6645362   3.8070085   1.2683201\n",
            "   2.0800416   3.5228188   0.37709713  4.1284103   1.057671    2.4205563\n",
            "   1.4905651   4.198494    2.3533027   4.3400226   3.7047052   3.2678409\n",
            "   0.9857964   1.3608086   3.167376   -0.3692389  -4.102768    2.7163558\n",
            "   4.2851305   3.0086925  -3.5234869   2.9845474   1.8320252   4.486533\n",
            "   2.4061007   2.8553648   3.516859    4.6381817  -0.53600687 -0.3781688\n",
            "   3.3813798   0.18862183  2.478893    3.4610536   1.1675754   1.1752084\n",
            "  -3.645712    0.15163517  2.923687    3.364152    3.6768217   3.6833923\n",
            "   3.5574      0.29648066  3.848738    2.3930557   3.65237     0.31485274\n",
            "   3.4483144   1.0501566   5.1726246   4.6832275   3.7414908   2.671827\n",
            "   3.1540859  -2.5072925   4.236003   -1.5004845   0.9921966  -1.8898435\n",
            "  -3.4999657   2.112963    1.6748025   2.7386448   2.8739932   0.42394182\n",
            "   0.32362357 -0.08249542  2.0414925   3.6237738   0.9081292  -2.5221317\n",
            "   2.0948734   4.281339    3.9090211   3.3885055   3.5916064   2.6537104 ]\n",
            " [ 3.598827    4.16162     3.6142542   3.630438    4.5076404   3.7222953\n",
            "   3.3418245  -2.985186    4.1128955   2.632007   -2.9891372   0.7343555\n",
            "   2.6504712   0.6078992  -0.54558617  3.0211623   1.8818957   2.1231797\n",
            "   0.02353118  0.47416675 -2.1874282   2.2934847   1.7314514   4.89222\n",
            "   2.718297    3.718615   -1.0604994   3.764696    3.957654    0.5823126\n",
            "   1.0858127   3.3675885   1.4824581   4.4642634   2.1629884   1.9700713\n",
            "   1.2017661   4.5371633   2.9834418   4.14338     3.5588548   3.544575\n",
            "   0.8122421   1.8177004   2.8195815  -0.04258473 -3.0691395   3.1985784\n",
            "   3.9962723   3.0203362  -2.2102811   3.233363    2.4243865   4.242187\n",
            "   2.8406286   2.3622444   3.4035585   4.431057    0.5955627   1.092627\n",
            "   2.4760575   0.56648946  2.4926708   3.4913454   3.200076    1.2467575\n",
            "  -2.261241    1.0213145   2.8651586   3.096863    2.9879942   3.706057\n",
            "   3.270685    1.123984    3.790923    2.579435    3.4913442   0.6498645\n",
            "   3.62189     1.3018553   5.3554626   4.726757    3.5062673   2.2722018\n",
            "   2.7886147  -1.7015896   3.402316   -1.711791    0.09636379 -1.5283803\n",
            "  -3.096293    1.7927785   2.1213906   2.9983547   2.160011    0.6396597\n",
            "   1.2655485   0.09214341  2.4204652   4.6072893   1.343639   -1.4040488\n",
            "   1.8025631   4.3385553   4.1241198   3.9589956   3.4528153   2.6488192 ]\n",
            " [ 3.6821384   3.8205245   3.9829574   3.9513855   3.912756    4.088804\n",
            "   3.8972719  -3.4912975   4.1875863   2.2998166  -2.6444533   1.1673199\n",
            "   2.5942264   0.16195172 -0.07645274  3.645591    1.8846407   1.6583524\n",
            "   0.85622406  0.7527497  -2.310513    2.1356509   1.6401976   4.7682843\n",
            "   3.0128362   3.6226282  -1.0918276   3.9170349   3.798642    1.202417\n",
            "   2.1733449   4.0675583   0.37065887  4.523327    1.4842447   2.8854003\n",
            "   1.6480358   4.2983627   2.494839    4.4353795   3.2379944   3.5247862\n",
            "   1.4093889   2.1883576   3.4201167   0.13365799 -3.6593673   3.0204604\n",
            "   4.183561    3.2745864  -2.798268    3.6604245   2.0630534   4.5370765\n",
            "   2.55104     2.9054978   3.5019855   5.007536   -0.1272712   0.01665888\n",
            "   3.1711187   0.20064807  2.591799    3.9908204   1.7381892   1.1806419\n",
            "  -2.8983955   0.6687835   2.8405733   3.494696    3.6082673   4.2041593\n",
            "   3.7297843   0.09099831  3.5545692   2.6449876   4.1748533   0.6239152\n",
            "   3.9280002   1.6550152   5.1485844   5.2518916   3.8111353   3.0325477\n",
            "   3.5417526  -1.833462    4.2252355  -1.4689498   1.1719339  -2.2657685\n",
            "  -2.9317055   2.6003819   2.0885513   3.7837968   3.398936    0.59134674\n",
            "   0.84151226  0.01414459  2.4032924   3.964793    1.8667641  -2.3542762\n",
            "   2.593465    4.5285563   4.155188    3.8622675   3.7183573   2.6302834 ]\n",
            " [ 3.1724913   3.4576561   3.3799632   3.1366744   3.7493443   3.631988\n",
            "   2.7576692  -4.002727    3.4952574   2.1375158  -2.3188732   1.3606784\n",
            "   2.0733442  -0.51133084 -0.6467228   3.1155248   2.2752032   0.96846384\n",
            "  -0.01438507  1.074363   -2.60723     1.8071966   1.5870351   4.1509767\n",
            "   2.7735589   3.0557897  -1.3140081   3.0607526   3.296927    1.188505\n",
            "   1.6769781   3.292406    0.39096057  3.8279595   0.77940655  2.08158\n",
            "   1.4793814   3.8701243   2.1160944   3.6818924   3.1819315   3.0314689\n",
            "   0.72615933  1.1873026   2.832289   -0.36553854 -4.109624    2.3991563\n",
            "   3.8210204   2.5552938  -3.1365395   2.768636    1.550501    3.8387659\n",
            "   2.0716126   2.3551521   3.4824314   4.131145   -0.89821494 -0.7189617\n",
            "   2.9261074   0.2210637   1.9931198   3.046258    0.6866649   1.4004681\n",
            "  -3.8008072   0.07462648  2.621168    2.9866996   3.093177    2.9714997\n",
            "   3.0058665   0.094727    3.264602    2.1040156   3.2021122   0.3274815\n",
            "   3.3412178   0.6880836   4.636837    4.1284637   3.2825518   2.3168378\n",
            "   2.6793313  -2.6317117   3.5080066  -1.362792    1.0329368  -1.7976221\n",
            "  -3.1318269   1.9675407   1.5435245   2.6717856   2.433146    0.2873137\n",
            "   0.12861432 -0.00860274  1.8406607   3.3012161   0.85106814 -1.9101222\n",
            "   1.7966759   4.0264053   3.8566813   3.1483932   3.153413    2.402228  ]]\n",
            "KKKKKKKKKKKKKKKKKKKKKKKKKK\n",
            "[[ 3.14200123 -4.24168481  3.50820838  3.45301098 -0.82878683  3.55198812\n",
            "  -2.39865244 -6.12672762 -0.10672683  0.71976341 -3.14990379  0.88687742\n",
            "   1.96307042 -2.96602867 -0.96485306  3.0573246   1.80836617  0.69483106\n",
            "  -3.59916629  0.88428494 -4.14375598  1.32681998  1.15791418  3.10145464\n",
            "  -3.94394818 -0.45704923 -3.17973647  2.01345678  3.49290798  1.08630887\n",
            "   1.91519355  1.99006189 -0.89089204  3.10862606  0.31465888  1.96238638\n",
            "   1.18530034  3.04548214  0.68146498  3.21140606  3.70074931 -0.6222812\n",
            "   0.33585268 -0.49023033  2.30214294 -1.62807221 -4.09654536  2.57267535\n",
            "   4.62704694  3.50130149 -4.55075547  2.32779749 -3.0686732   2.30381479\n",
            "   2.52119496  3.81951609  3.41332524  3.67610632 -6.74182502 -5.01156443\n",
            "   1.5462474  -1.05207687  1.35996932  2.44520436  0.37169979 -0.35820057\n",
            "  -5.11575173 -0.6314072   0.32288053  1.46943856  2.76690953  3.37200363\n",
            "   1.7394532  -6.58334416  3.5319249   1.6127086   3.13503207 -0.66395155\n",
            "   4.06999663  1.15799549  4.73989636  4.31418599  1.44526233  0.28127454\n",
            "   2.64671056 -4.54198503  3.74632638 -3.03609444  1.52410275 -3.03882508\n",
            "  -2.73518963  1.30597321  2.11105062  2.67127132  3.57314248 -1.12735046\n",
            "  -1.6723929  -0.46496717  0.65868609  4.05736274  0.85526518 -2.72253459\n",
            "   3.54003411  5.17118197  5.18668262  3.66831296  3.82868818  1.1772571 ]\n",
            " [ 3.40599731  5.7134509   5.25723885  5.24045097  4.74172481  4.90486413\n",
            "   5.09260533 -5.02544379  5.72105329  2.18549181 -3.21533712  1.64194021\n",
            "   2.90787944  0.80145692 -0.59077416  5.30981217  2.28761019  0.42374435\n",
            "   2.7811058   0.78562168 -3.47333691  2.90658546  2.36947666  6.32784573\n",
            "   2.86011799  4.23956907  0.03411955  3.819755    4.24397878  2.2907991\n",
            "   0.21054544  3.32214512 -1.90970399  4.45317078  0.6683813   1.59365058\n",
            "   1.68440314  5.11113288  2.99692001  5.99923782  6.64309868  5.40295846\n",
            "   0.32490177  0.69211005  3.29979136 -1.60424629 -6.62905308  4.90352694\n",
            "   4.99688408  4.12495816 -2.9216337   3.43068149  2.60198514  5.95873753\n",
            "   2.83167517  4.34509765  3.82192717  6.32176238  0.12217197  2.80634905\n",
            "  -0.86734911 -0.21511189  3.53131945  2.87442117 -1.92782223  0.3331608\n",
            "  -4.27693843  0.93958593  1.16395248  3.26064057  3.99138537  5.40150642\n",
            "   4.2357398   2.43908912  5.01012806  3.44314736  5.89294117  1.61704812\n",
            "   4.24549075 -1.43617676  6.00028627  6.64315603  4.89968855  5.1618463\n",
            "   2.60143546 -2.46206637  2.71564655 -3.14757091  0.1206718  -2.06879665\n",
            "  -5.83264703  1.05089459  1.72550644  2.60879083  2.79398982  0.15725611\n",
            "  -1.78106163  1.14279554  1.38952026  3.71189082  0.87435186 -3.04563706\n",
            "   0.97094974  5.29104355  4.50302929  3.90080892  3.59288026  2.2498049 ]\n",
            " [ 1.96404004  4.18793665  3.72307462  3.83831099  4.63164818  4.29094029\n",
            "   3.4738469  -4.31800609  4.68391297 -3.67316199 -3.48294623  1.92746715\n",
            "   2.78179132  1.07379666 -2.47412202  3.75153801  1.94690928  1.64438166\n",
            "  -1.05622589  0.84979956  0.65234749  2.14098663  1.6881807   3.44959609\n",
            "   2.47116382  3.41513531 -4.56660239  3.49501202  4.12248746  1.18194548\n",
            "   1.56875381  3.12870737 -0.09465791  2.94071492  3.7030081   1.54371028\n",
            "   1.45652041  3.72899383  1.16979546  2.96443568  3.50762486  4.30596366\n",
            "   1.73265795 -0.2373696   3.86771317  0.27795573 -6.60204283  2.22274003\n",
            "   3.70114881  3.0719749  -3.3373347   1.24348104  2.45757206  4.25939791\n",
            "   2.27841533  4.65009102  4.31996536  4.93131786 -0.88341345  2.30582916\n",
            "   3.32505968  0.43437624  2.37306232  2.33701177  3.25836872  2.52399466\n",
            "  -1.52221761 -1.73670812  0.30648313  2.3340165   3.88535771  2.56101572\n",
            "   3.81873056  1.79502943  3.9830403   2.05885304  3.86123095  0.0996433\n",
            "   3.58405557  3.34044943  4.60574901  5.19640043  4.49756673  0.0451981\n",
            "   3.1478115  -0.85689185  3.82579857 -1.01961361 -0.60759911 -1.36907289\n",
            "  -5.10694932  2.06394732 -0.19665815  4.33187305  2.32489912  2.03414425\n",
            "  -1.51660601  0.0778435   2.17912698  2.44180358  2.16433849 -3.04563706\n",
            "   2.07754174  5.10115296  3.53567373  3.310621    3.14621541  3.4110415 ]\n",
            " [ 3.97034555  4.13282077  4.06707016  4.06707016  4.73155874  3.57345417\n",
            "   2.97094164 -6.23353954  4.66349273 -3.81545965 -3.80542116 -0.02067543\n",
            "   2.10773558 -1.06321623 -1.56548203  3.46852266  1.93259436  2.03527569\n",
            "  -1.85720091  0.56094892 -1.71324337  2.36090763  1.1730168   5.01259902\n",
            "   2.9101527   3.34695904 -4.82040379  3.25860411  3.39394204 -0.47723341\n",
            "   2.21792906  4.37570536 -1.2432081   4.60162269  2.52824941  2.65555532\n",
            "   1.36568875  3.24100929  0.48286891  3.80629973  0.26357077  2.60204402\n",
            "   1.38337028  0.58411234  2.32582362  0.2366228  -5.16535804  3.23211608\n",
            "   2.75761827  1.86304508 -5.90303888  2.14381046  1.6370648   3.82386053\n",
            "   2.85309965  2.02289612  4.39409646  5.25221278 -1.4240235  -4.16905397\n",
            "   3.81804257 -0.55858914  2.46374897  2.92741851  3.68474093  1.16584812\n",
            "  -5.77073819 -0.25100607  0.85601037  2.91951657  3.02739844  3.26521823\n",
            "   4.51328171  0.10055934  3.53983522  2.23291559  2.69185404  0.36491857\n",
            "   2.75015526  2.81670925  5.57038255  4.52333132  3.00985135  0.02592262\n",
            "   2.47590801 -3.07918527  1.61990612 -2.02149623 -0.3763153  -2.40425876\n",
            "  -4.13383804  2.33055436  1.15092155  2.86605652  1.94299307  1.97329928\n",
            "  -0.70299578  0.05804601  2.0794838   3.12370469  2.39192988 -4.27874828\n",
            "   1.16356229  3.80816458  3.12546865  4.26710199  4.49749879  3.9741751 ]]\n",
            "KKKKKKKKKKKKKKKKKKKKKKKKKK\n",
            "i will be here /tmp/yi_cu8wa.h5\n",
            "100% 2/2 [00:31<00:00, 15.74s/it, best loss: 2.5276222229003906]\n",
            "am here\n",
            "Best performing model chosen hyper-parameters:\n",
            "{'batch_size': 1, 'l1_dropout': 0.2982348031910509, 'layer_1_size': 68.0, 'optimizer': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/DeepCDR-master/prog/run_DeepCDR_allmodels.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y40IM3PTzE5",
        "outputId": "648307ba-9569-466e-f6bb-2a0f8962c2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    import argparse\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import random, os, sys\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import csv\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from scipy import stats\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import time\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn import metrics\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.metrics import roc_auc_score\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn import preprocessing\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras.backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Model, Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import load_model\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Input, InputLayer, Multiply, ZeroPadding2D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Conv2D, MaxPooling2D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dense, Activation, Dropout, Flatten, Concatenate\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import BatchNormalization\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Lambda\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import optimizers, utils\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.constraints import max_norm\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import regularizers\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, History\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.optimizers import Adam, SGD\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import model_from_json\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import tensorflow as tf\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.metrics import average_precision_score\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.metrics import mean_squared_error, r2_score\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from scipy.stats import pearsonr\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from funs_hypetune_cdr import *\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import scipy.sparse as sp\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import argparse\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib.pyplot as plt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras.backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Model, Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Input, InputLayer, Multiply, ZeroPadding2D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dense, Activation, Dropout, Flatten, Concatenate\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import BatchNormalization\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Lambda\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dropout, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Model\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.optimizers import Adam\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.regularizers import l2\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from layers.graph import GraphLayer, GraphConv\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import RepeatedKFold\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform, quniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import tempfile\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'layer_1_size': hp.quniform('layer_1_size', 12, 256, 4),\n",
            "        'l1_dropout': hp.uniform('l1_dropout', 0.001, 0.7),\n",
            "        'optimizer': hp.choice('optimizer', ['adam']),\n",
            "    }\n",
            "\n",
            ">>> Functions\n",
            "  1: def compute_accuracy(y_true, y_pred):\n",
            "  2:     pred = y_pred.ravel() < 0.5\n",
            "  3:     return np.mean(pred == y_true)\n",
            "  4: \n",
            "  5: \n",
            ">>> Data\n",
            "  1: \n",
            "  2: DPATH = '/content/DeepCDR-master/data'  \n",
            "  3: \n",
            "  4: Genomic_mutation_file = '%s/CCLE/genomic_mutation_34673_demap_features_auto.csv'%DPATH\n",
            "  5: \n",
            "  6: Gene_expression_file = '%s/CCLE/genomic_expression_561celllines_697genes_demap_features_auto.csv'%DPATH\n",
            "  7: Methylation_file = '%s/CCLE/genomic_methylation_561celllines_808genes_demap_features_auto.csv'%DPATH\n",
            "  8: mutation_feature, gexpr_feature,methylation_feature, Y = MetadataGenerate(Genomic_mutation_file,Gene_expression_file,Methylation_file,False)\n",
            "  9: \n",
            " 10: X2=np.matrix(mutation_feature)\n",
            " 11: X3=np.matrix(gexpr_feature)\n",
            " 12: X4=np.matrix(methylation_feature)\n",
            " 13: Y=np.matrix(Y)\n",
            " 14: X=[X2,X3,X4]\n",
            " 15: \n",
            " 16: \n",
            " 17: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     \"\"\"\n",
            "   4:     Model providing function:\n",
            "   5: \n",
            "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
            "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
            "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
            "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
            "  10:     The last one is optional, though recommended, namely:\n",
            "  11:         - model: specify the model just created so that we can later use it again.\n",
            "  12:     \"\"\"\n",
            "  13:     print(\"AM BACK\")\n",
            "  14:     regr=True\n",
            "  15:     layer_1_size = space['layer_1_size']\n",
            "  16:     l1_dropout = space['l1_dropout']\n",
            "  17:     params = {\n",
            "  18:         'l1_size': layer_1_size,\n",
            "  19:         'l1_dropout': l1_dropout\n",
            "  20:     }\n",
            "  21:     print(X[0].shape[-1])\n",
            "  22:     print(Y.shape[-1])\n",
            "  23:     print(\"??????????????????????\")\n",
            "  24:     mut_input = Input(shape=(X[0].shape[-1],))\n",
            "  25:     gexpr_input = Input(shape=(X[1].shape[-1],))\n",
            "  26:     methy_input = Input(shape=(X[2].shape[-1],))\n",
            "  27:     \n",
            "  28:     y_dim=Y.shape[1]\n",
            "  29:     \n",
            "  30:     x_mut = Dense(16)(mut_input)\n",
            "  31:     x_mut = Activation('tanh')(x_mut)\n",
            "  32:     x_mut = BatchNormalization()(x_mut)\n",
            "  33:     x_mut = Dropout(0.1)(x_mut)\n",
            "  34:     x_mut = Dense((y_dim),activation='relu')(x_mut)\n",
            "  35:         #gexp feature\n",
            "  36:     x_gexpr = Dense(16)(gexpr_input)\n",
            "  37:     x_gexpr = Activation('tanh')(x_gexpr)\n",
            "  38:     x_gexpr = BatchNormalization()(x_gexpr)\n",
            "  39:     x_gexpr = Dropout(0.1)(x_gexpr)\n",
            "  40:     x_gexpr = Dense((y_dim),activation='relu')(x_gexpr)\n",
            "  41:         #methylation feature\n",
            "  42:     x_methy = Dense(16)(methy_input)\n",
            "  43:     x_methy = Activation('tanh')(x_methy)\n",
            "  44:     x_methy = BatchNormalization()(x_methy)\n",
            "  45:     x_methy = Dropout(0.1)(x_methy)\n",
            "  46:     x_methy = Dense((y_dim),activation='relu')(x_methy)\n",
            "  47:     \n",
            "  48:     x = Concatenate()([x_mut,x_gexpr,x_methy])\n",
            "  49:     x = Dense(300,activation = 'relu')(x)\n",
            "  50:     x = Dropout(0.1)(x)\n",
            "  51:     x = Lambda(lambda x: K.expand_dims(x,axis=-1))(x)\n",
            "  52:     x = Lambda(lambda x: K.expand_dims(x,axis=1))(x)\n",
            "  53:     x = Conv2D(filters=30, kernel_size=(1,150),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
            "  54:     x = MaxPooling2D(pool_size=(1,2))(x)\n",
            "  55:     x = Conv2D(filters=10, kernel_size=(1,5),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
            "  56:     x = MaxPooling2D(pool_size=(1,3))(x)\n",
            "  57:     x = Conv2D(filters=5, kernel_size=(1,5),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
            "  58:     x = MaxPooling2D(pool_size=(1,3))(x)\n",
            "  59:     x = Dropout(0.1)(x)\n",
            "  60:     x = Flatten()(x)\n",
            "  61:     x = Dropout(0.1)(x)\n",
            "  62:     if regr:\n",
            "  63:       output = Dense((y_dim),name='output')(x)\n",
            "  64:     else:\n",
            "  65:       output = Dense(1,activation = 'sigmoid',name='output')(x)\n",
            "  66:     #model  = Model(inputs=[mut_input,gexpr_input,methy_input],outputs=output)  \n",
            "  67:     #model.compile(loss='mean_squared_error', metrics=['mse'],optimizer=space['optimizer'])\n",
            "  68:     x=[X[0],X[1],X[2]]\n",
            "  69:     score=[]\n",
            "  70:     #models=get_models()\n",
            "  71:     #print(models)\n",
            "  72:     #for model in models:\n",
            "  73: \t# evaluate model using each test condition\n",
            "  74: \t  #cv_mean = evaluate_model(cv, model)\n",
            "  75:     model=SVC()\n",
            "  76:     result = model.fit(x,Y)\n",
            "  77:     #get the highest validation accuracy of the training epochs\n",
            "  78:     x_test=[X[0],X[1],X[2]]\n",
            "  79:     print(\"RRRRRRRRRRRRRRRRRRR\")\n",
            "  80:     mse=model.evaluate(x,Y, verbose=0)\n",
            "  81:     score.append(mse)\n",
            "  82:     #y_pred = model.predict([x_test], verbose=0)\n",
            "  83:     #DF1 = pd.DataFrame(y_pred)\n",
            "  84:     #DF1.to_csv(\"pred_CDR.csv\")\n",
            "  85:     print(\"KKKKKKKKKKKKKKKKKKKKKKKKKK\")\n",
            "  86:     #print(Y_test)\n",
            "  87:     #DF2 = pd.DataFrame(Y)\n",
            "  88:     #DF2.to_csv(\"real_CDR.csv\")\n",
            "  89:     #tr_acc = compute_accuracy(Y_test, y_pred)\n",
            "  90:     \n",
            "  91:     #print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
            "  92:    \n",
            "  93:     \n",
            "  94:     \n",
            "  95:     out = {\n",
            "  96:         \n",
            "  97:         'loss': mse[0],\n",
            "  98:         'status': STATUS_OK,\n",
            "  99:         'model_params': params,\n",
            " 100:     }\n",
            " 101:     #print(overall_pcc)\n",
            " 102:     print(\"KKKKKKKKKKKKKKKKKKKKKKKKKK\")\n",
            " 103:     temp_name = tempfile.gettempdir()+'/'+next(tempfile._get_candidate_names()) + '.h5'\n",
            " 104:     #model.save(temp_name)\n",
            " 105:     with open(temp_name, 'rb') as infile:\n",
            " 106:         model_bytes = infile.read()\n",
            " 107:     out['model_serial'] = model_bytes\n",
            " 108:     print(\"i will be here %s\"%temp_name)\n",
            " 109:     print(score)\n",
            " 110:     return out\n",
            " 111: \n",
            "\r  0% 0/1 [00:00<?, ?it/s, best loss: ?]\r                                       \rAM BACK\n",
            "\r  0% 0/1 [00:00<?, ?it/s, best loss: ?]\r                                       \r26\n",
            "\r  0% 0/1 [00:00<?, ?it/s, best loss: ?]\r                                       \r108\n",
            "??????????????????????\n",
            "  0% 0/1 [00:00<?, ?it/s, best loss: ?]2021-12-28 16:23:16.830196: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "  0% 0/1 [00:00<?, ?it/s, best loss: ?]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DeepCDR-master/prog/run_DeepCDR_allmodels.py\", line 200, in <module>\n",
            "    keep_temp=True)  # this last bit is important\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hyperas/optim.py\", line 69, in minimize\n",
            "    keep_temp=keep_temp)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hyperas/optim.py\", line 139, in base_minimizer\n",
            "    return_argmin=True),\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hyperopt/fmin.py\", line 388, in fmin\n",
            "    show_progressbar=show_progressbar,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hyperopt/base.py\", line 639, in fmin\n",
            "    show_progressbar=show_progressbar)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hyperopt/fmin.py\", line 407, in fmin\n",
            "    rval.exhaust()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hyperopt/fmin.py\", line 262, in exhaust\n",
            "    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hyperopt/fmin.py\", line 227, in run\n",
            "    self.serial_evaluate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hyperopt/fmin.py\", line 141, in serial_evaluate\n",
            "    result = self.domain.evaluate(spec, ctrl)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hyperopt/base.py\", line 844, in evaluate\n",
            "    rval = self.fn(pyll_rval)\n",
            "  File \"./temp_model.py\", line 353, in keras_fmin_fnct\n",
            "    result = model.fit(x,Y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\", line 196, in fit\n",
            "    accept_large_sparse=False,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 576, in _validate_data\n",
            "    X, y = check_X_y(X, y, **check_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 968, in check_X_y\n",
            "    estimator=estimator,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 738, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\n",
            "    return array(a, dtype, copy=False, order=order)\n",
            "ValueError: could not broadcast input array from shape (34,26) into shape (34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hnwtDPeQpHfB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Fine_HyperTune_Multi_Omics_MultiReg_CDR_DNN_DrugResponse.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}